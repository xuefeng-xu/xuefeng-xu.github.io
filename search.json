[
  {
    "objectID": "teach/teach.html",
    "href": "teach/teach.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "@ University of Warwick\n\nCS429/CS909 Data Mining: Spring 2025\nCS133 Professional Skills: Fall 2024\n\n\n\n@ Beihang University\n\n39112201 Abstract Algebra: Fall 2019"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "University of Warwick | Coventry, UK Ph.D. in Computer Science | September 2024 - Present\n\n\nBeihang University | Beijing, China M.E. in Cyberspace Security | September 2019 - January 2022\n\n\nJilin University | Changchun, China B.E. in Communication Engineering | September 2015 - July 2019"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "University of Warwick | Coventry, UK Ph.D. in Computer Science | September 2024 - Present\n\n\nBeihang University | Beijing, China M.E. in Cyberspace Security | September 2019 - January 2022\n\n\nJilin University | Changchun, China B.E. in Communication Engineering | September 2015 - July 2019"
  },
  {
    "objectID": "cv.html#experience",
    "href": "cv.html#experience",
    "title": "Xuefeng Xu",
    "section": "Experience",
    "text": "Experience\n\n\nPrimiHub | Beijing, China Privacy Engineer | December 2022 - August 2024\n\n\nOPPO | Shenzhen, China Machine Learning Intern | May 2022 - August 2022"
  },
  {
    "objectID": "cv.html#research",
    "href": "cv.html#research",
    "title": "Xuefeng Xu",
    "section": "Research",
    "text": "Research\n\n\nFedPS: Federated data Preprocessing via aggregated Statistics Code\n\n\nDeep Learning Algorithms Design and Implementation Based on Differential Privacy Xuefeng Xu, Yanqing Yao, and Lei Cheng International Conference on Machine Learning for Cyber Security, 2020"
  },
  {
    "objectID": "cv.html#teaching",
    "href": "cv.html#teaching",
    "title": "Xuefeng Xu",
    "section": "Teaching",
    "text": "Teaching\n\n@ University of Warwick\n\nCS429/CS909 Data Mining: Spring 2025\nCS133 Professional Skills: Fall 2024\n\n\n\n@ Beihang University\n\n39112201 Abstract Algebra: Fall 2019"
  },
  {
    "objectID": "cv.html#service",
    "href": "cv.html#service",
    "title": "Xuefeng Xu",
    "section": "Service",
    "text": "Service\n\nConference Reviewer\n\nInternational Conference on Learning Representations (ICLR) 2025"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPower Transform\n\n\n\nStatistics\n\nPython\n\n\n\nBox-Cox and Yeo-Johnson are power transformations that normalize data but require careful handling to avoid numerical instability.\n\n\n\n\n\nApr 14, 2025\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\nMonotone Piecewise Cubic Interpolation\n\n\n\nStatistics\n\n\n\nPCHIP is a cubic interpolation method that preserves monotonicity by computing shape-preserving derivatives to avoid overshoots.\n\n\n\n\n\nMar 13, 2025\n\n3 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Xuefeng Xu 许雪峰",
    "section": "",
    "text": "Email: xuefeng.xu@warwick.ac.uk\n \n  \n   \n  \n    \n     GitHub\n  \n  \n      Google Scholar\n  \n\n\n\n\nBiography\nI am a Ph.D. student in the Department of Computer Science at the University of Warwick, supervised by Prof. Graham Cormode. I also work with Prof. Hakan Ferhatosmanoglu. My research focuses on Privacy and Security, particularly Differential Privacy and Machine Learning. I hold an M.E. degree from Beihang University and a B.E. degree from Jilin University.\n\n\nNews\n\n03/2025: Online Talk at Flower AI Summit 2025"
  },
  {
    "objectID": "misc/resource.html",
    "href": "misc/resource.html",
    "title": "Study Resources",
    "section": "",
    "text": "Books\n\nIntroduction to Linear Algebra\nAlgorithms\nNumerical Recipes: The Art of Scientific Computing\nPattern Recognition and Machine Learning\nThe Elements of Statistical Learning\nProbabilistic Machine Learning\nFoundations of Machine Learning\nThe Algorithmic Foundations of Differential Privacy\nThe Complexity of Differential Privacy\nIntroduction to Modern Cryptography\nEffective Python\n\n\n\nCourses\n\nMIT 18.06 Linear Algebra\nCoursera Algorithms Part 1 and Part 2\nStanford CS229 Machine Learning\nCoursera Deep Learning Specialization\nCoursera Cryptography\nMIT The Missing Semester of Your CS Education"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "FedPS: Federated data Preprocessing via aggregated Statistics Code\n\n\nDeep Learning Algorithms Design and Implementation Based on Differential Privacy Xuefeng Xu, Yanqing Yao, and Lei Cheng International Conference on Machine Learning for Cyber Security, 2020"
  },
  {
    "objectID": "research/research.html",
    "href": "research/research.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "FedPS: Federated data Preprocessing via aggregated Statistics Code\n\n\nDeep Learning Algorithms Design and Implementation Based on Differential Privacy Xuefeng Xu, Yanqing Yao, and Lei Cheng International Conference on Machine Learning for Cyber Security, 2020"
  },
  {
    "objectID": "misc/index.html",
    "href": "misc/index.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "Stop PP10043 !\nStudy Resources"
  },
  {
    "objectID": "blog/pchip.html",
    "href": "blog/pchip.html",
    "title": "Monotone Piecewise Cubic Interpolation",
    "section": "",
    "text": "Piecewise Cubic Hermite Interpolating Polynomial (PCHIP) is a cubic spline-based interpolation method designed to preserve monotonicity. See MATLAB or SciPy for the implementation details."
  },
  {
    "objectID": "blog/pchip.html#interpolation-function",
    "href": "blog/pchip.html#interpolation-function",
    "title": "Monotone Piecewise Cubic Interpolation",
    "section": "1 Interpolation Function",
    "text": "1 Interpolation Function\nGiven n data points (x_1,y_1),\\dots,(x_n,y_n) with x_1&lt;\\cdots&lt;x_n, where y is monotonic (either y_i\\le y_{i+1} or y_{i+1}\\ge y_i), define:\n\nh_i=x_{i+1}-x_i,\\ s_i=\\frac{y_{i+1}-y_i}{h_i}\n\\tag{1}\nFor x_i&lt;x&lt;x_{i+1}, the interpolation function f(x) is a cubic polynomial:\n\nf(x)=c_0+c_1(x-x_i)+c_2(x-x_i)^2+c_3(x-x_i)^3\n\\tag{2}\nsatisfying:\n\n\\begin{align*}\nf(x_i)&=y_i, &f(x_{i+1})&=y_{i+1}\\\\\nf'(x_i)&=d_i, &f'(x_{i+1})&=d_{i+1}\n\\end{align*}\n\\tag{3}\nSolving for the coefficients:\n\n\\begin{align*}\nc_0&=y_i, &c_2&=\\frac{3s_i-2d_i-d_{i+1}}{h_i}\\\\\nc_1&=d_i, &c_3&=\\frac{-2s_i+d_i+d_{i+1}}{h_i^2}\\\\\n\\end{align*}\n\\tag{4}\nThus, computing derivatives d_1,\\dots,d_n determines c_0,c_1,c_2,c_3 for each interval."
  },
  {
    "objectID": "blog/pchip.html#derivative-computation",
    "href": "blog/pchip.html#derivative-computation",
    "title": "Monotone Piecewise Cubic Interpolation",
    "section": "2 Derivative Computation",
    "text": "2 Derivative Computation\nThe derivative at x_i is computed using local information from three neighboring points (Fritsch and Butland 1984):\n\nd_i=G(s_{i-1},s_i,h_{i-1},h_i)=\n\\begin{cases}\n\\frac{s_{i-1}s_i}{rs_i+(1-r)s_{i-1}} & \\mathrm{if~}s_{i-1}s_i&gt;0, \\\\\n0 & \\text{otherwise} &\n\\end{cases}\n\\tag{5}\nwhere the ratio r (1/3&lt;r&lt;2/3) is given by:\n\nr=\\frac{h_{i-1}+2h_i}{3h_{i-1}+3h_i}=\\frac{1}{3}\\Big(1+\\frac{h_i}{h_{i+1}+h_i}\\Big)\n\\tag{6}\nIf s_{i-1} and s_i have opposite signs (indicating non-monotonicity) or one is zero, then d_i=0. Otherwise, d_i is a weighted harmonic mean:\n\n\\frac{1}{d_i}=\\frac{r}{s_{i-1}}+\\frac{1-r}{s_i}\n\\tag{7}\nFor endpoints x_1 and x_n, derivatives are computed separately (Moler 2004, chap. 3):\n\nd_1=\n\\begin{cases}\n0 & \\mathrm{if~}\\text{sgn}(\\hat{d}_1)\\neq \\text{sgn}(s_1), \\\\\n3s_1 & \\mathrm{if~}\\text{sgn}(s_1)\\neq \\text{sgn}(s_2) \\land|\\hat{d}_1|&gt;3|s_1|,\\\\\n\\hat{d}_1 & \\text{otherwise} &\n\\end{cases}\n\\tag{8}\nwhere:\n\n\\hat{d}_1=\\frac{(2h_1+h_2)s_1 - h_1s_2}{{h_1+h_2}}\n\\tag{9}\nA quadratic polynomial \\hat{f}(x)=\\hat{c}_0+\\hat{c}_1x+\\hat{c}_2x^2 is fit through the first three points, and its derivative at x_1 is computed to obtain \\hat{d}_1. Additional rules are then applied to preserve monotonicity. Similar rules apply for d_n."
  },
  {
    "objectID": "blog/pchip.html#monotonicity-conditions",
    "href": "blog/pchip.html#monotonicity-conditions",
    "title": "Monotone Piecewise Cubic Interpolation",
    "section": "3 Monotonicity Conditions",
    "text": "3 Monotonicity Conditions\nTo ensure monotonicity, define:\n\n\\alpha_i=\\frac{d_i}{s_i},\\\n\\beta_i=\\frac{d_{i+1}}{s_i}\n\\tag{10}\n\nLemma 1 A sufficient condition for monotonicity is (Fritsch and Carlson 1980):\n\n\\alpha_i,\\beta_i\\ge0\n\\land\n\\Big(\n\\alpha_i,\\beta_i\\le3\n\\lor\n\\phi(\\alpha_i,\\beta_i)\\ge0\n\\Big)\n\\tag{11}\nwhere:\n\n\\phi(\\alpha,\\beta)=\\alpha-\\frac{(2\\alpha+\\beta-3)^2}{3(\\alpha+\\beta-2)}\n\\tag{12}\n\nIf s_{i-1}s_i&gt;0, then \\alpha_i&gt;0; otherwise, \\alpha_i=0. Since the ratio r satisfies 1/3&lt;r&lt;2/3, \\alpha_i is upper-bounded by 3:\n\n\\alpha_i\n=\\frac{1}{rs_i/s_{i-1}+(1-r)}\n&lt;\\frac{1}{1-r}&lt;3\n\\tag{13}\nFor endpoint \\alpha_1, we only need to show the condition of \\text{sgn}(\\hat{d}_1)=\\text{sgn}(s_1)=\\text{sgn}(s_2), since other conditions already lie within the region [0,3].\n\n\\alpha_1\n=1+\\frac{1-s_2/s_1}{1+h_2/h_1}&lt;2\n\\tag{14}\nSimilarly, \\beta_i and endpoint \\beta_{n-1} all satisfy the monotonicity condition."
  },
  {
    "objectID": "blog/pchip.html#proof-of-monotonicity",
    "href": "blog/pchip.html#proof-of-monotonicity",
    "title": "Monotone Piecewise Cubic Interpolation",
    "section": "4 Proof of Monotonicity",
    "text": "4 Proof of Monotonicity\n\nProof. To preserve monotonicity, the derivatives d_i and d_{i+1} must align with the direction of the slope of the interval s_i. This is a necessary condition (Fritsch and Carlson 1980):\n\n\\text{sgn}(d_i)=\\text{sgn}(d_{i+1})=\\text{sgn}(s_i)\n\\Leftrightarrow\n\\alpha_i,\\beta_i\\ge0\n\\tag{15}\nThe derivative of f(x) is a quadratic polynomial:\n\nf'(x)=c_1+\n2c_2(x-x_i)+\n3c_3(x-x_i)^2\n\\tag{16}\nIt has a unique extremum at:\n\nx^*=x_i+\\frac{h_i}{3}\\cdot\\frac{2\\alpha_i+\\beta_i-3}{\\alpha_i+\\beta_i-2}\n\\tag{17}\nand\n\nf'(x^*)=\\phi(\\alpha_i,\\beta_i)s_i\n\\tag{18}\nThere are three conditions to check: 1) x^*&lt;x_i; 2) x^*&gt;x_{i+1}; 3) x_i\\le x^*\\le x_{i+1}.\nCondition 1) is equivalent to \n\\alpha_i,\\beta_i\\ge0\n\\land\n\\frac{2\\alpha_i+\\beta_i-3}{\\alpha_i+\\beta_i-2}&lt;0\n\\tag{19}\nThe analysis of Condition 2) and 3) are similar, leading to:\n\n\\alpha_i,\\beta_i\\ge0\n\\land\n\\frac{\\alpha_i+2\\beta_i-3}{\\alpha_i+\\beta_i-2}&lt;0\n\\tag{20}\nand\n\n\\alpha_i,\\beta_i\\ge0\n\\land\n\\frac{2\\alpha_i+\\beta_i-3}{\\alpha_i+\\beta_i-2}\\ge0\n\\land\n\\frac{\\alpha_i+2\\beta_i-3}{\\alpha_i+\\beta_i-2}\\ge0\n\\land\n\\phi(\\alpha_i,\\beta_i)\\ge0\n\\tag{21}\nFigure 1 illustrates these conditions separately: Condition 1) — blue, Condition 2) — green, Condition 3) — red.\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx_vals = np.linspace(0, 4, 1000)\ny_vals = np.linspace(0, 4, 1000)\nx, y = np.meshgrid(x_vals, y_vals)\n\ncond1 = (x &gt; 0) & (y &gt; 0) & ((2 * x + y - 3) / (x + y - 2) &lt; 0)\ncond2 = (x &gt; 0) & (y &gt; 0) & ((x + 2 * y - 3) / (x + y - 2) &lt; 0)\ncond3 = (\n    (x &gt; 0)\n    & (y &gt; 0)\n    & ((2 * x + y - 3) / (x + y - 2) &gt; 0)\n    & ((x + 2 * y - 3) / (x + y - 2) &gt; 0)\n    & (x - ((2 * x + y - 3) ** 2) / (3 * (x + y - 2)) &gt; 0)\n)\n\nfig, ax = plt.subplots(figsize=(4, 4))\n\nax.contourf(x, y, cond1, levels=1, colors=[\"white\", \"blue\"], alpha=1)\nax.contourf(x, y, cond2, levels=1, colors=[\"white\", \"green\"], alpha=0.5)\nax.contourf(x, y, cond3, levels=1, colors=[\"white\", \"red\"], alpha=0.25)\n\nax.set_xlabel(r\"$\\alpha$\")\nax.set_ylabel(r\"$\\beta$\")\nax.set_xlim([0, 4])\nax.set_ylim([0, 4])\nax.xaxis.set_ticks(list(np.arange(0, 4.5, 0.5)))\nax.yaxis.set_ticks(list(np.arange(0, 4.5, 0.5)))\n\nax.grid(True)\nax.tick_params(\n    bottom=True,\n    top=True,\n    left=True,\n    right=True,\n    labelbottom=True,\n    labeltop=True,\n    labelleft=True,\n    labelright=True,\n)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: The monotonicity region\n\n\n\n\n\n\nFinally, simplifying yields the final monotonicity condition (Lemma 1)."
  },
  {
    "objectID": "blog/pchip.html#cubic-hermite-interpolation",
    "href": "blog/pchip.html#cubic-hermite-interpolation",
    "title": "Monotone Piecewise Cubic Interpolation",
    "section": "5 Cubic Hermite Interpolation",
    "text": "5 Cubic Hermite Interpolation\nCubic Hermite Interpolation constructs f(x) using both function values and derivatives:\n\nf(x)=y_iH_1(x)+y_{i+1}H_2(x)+d_iH_3(x)+d_{i+1}H_4(x)\n\\tag{22}\nwhere basis functions are:\n\n\\begin{align*}\nH_{1}(x)&=\\phi\\Big(\\frac{x_{i+1}-x}{h_{i}}\\Big), &H_{2}(x)&=\\phi\\Big(\\frac{x-x_{i}}{h_{i}}\\Big)\\\\\nH_{3}(x)&=-h_{i}\\psi\\Big(\\frac{x_{i+1}-x}{h_{i}}\\Big), &H_{4}(x)&=h_{i}\\psi\\Big(\\frac{x-x_{i}}{h_{i}}\\Big)\n\\end{align*}\n\\tag{23}\nwith:\n\n\\phi(t)=3t^{2}-2t^{3},\\  \\psi(t)=t^{3}-t^{2}\n\\tag{24}\nThis formulation aligns with the cubic polynomial definition above."
  },
  {
    "objectID": "blog/power-transform.html",
    "href": "blog/power-transform.html",
    "title": "Power Transform",
    "section": "",
    "text": "Power transforms are parametric methods that convert data into a Gaussian-like distribution. Two widely used transformations in this category are the Box-Cox (Box and Cox 1964) and Yeo-Johnson (I. Yeo and Johnson 2000) methods, both of which rely on a single parameter \\lambda."
  },
  {
    "objectID": "blog/power-transform.html#two-transformations",
    "href": "blog/power-transform.html#two-transformations",
    "title": "Power Transform",
    "section": "1 Two Transformations",
    "text": "1 Two Transformations\nThe Box-Cox transformation requires strictly positive data (x &gt; 0) and is defined as:\n\n\\psi_{\\text{BC}}(\\lambda, x) =\n\\begin{cases}\n\\frac{x^\\lambda-1}{\\lambda} & \\text{if } \\lambda\\neq0,\\\\\n\\ln x & \\text{if } \\lambda=0.\n\\end{cases}\n\\tag{1}\nThe Yeo-Johnson transformation generalizes Box-Cox to handle non-positive values and is defined as:\n\n\\psi_{\\text{YJ}}(\\lambda, x) =\n\\begin{cases}\n\\frac{(x+1)^\\lambda-1}{\\lambda} & \\text{if } \\lambda\\neq0,x\\ge0,\\\\\n\\ln(x+1) & \\text{if } \\lambda=0,x\\ge0,\\\\\n-\\frac{(-x+1)^{2-\\lambda}-1}{2-\\lambda} & \\text{if } \\lambda\\neq2,x&lt;0,\\\\\n-\\ln(-x+1) & \\text{if } \\lambda=2,x&lt;0.\\\\\n\\end{cases}\n\\tag{2}\nFigure 1 visualizes these transformations across various \\lambda values.\n\n\nCode\nimport numpy as np\nfrom scipy.special import boxcox\nfrom scipy.stats import yeojohnson\nimport matplotlib.pyplot as plt\n\ndef power_plot(x_min, x_max, power):\n    if power == \"BC\":\n        power_func = boxcox\n    else:\n        power_func = yeojohnson\n\n    eps = 0.01\n    x = np.arange(x_min, x_max, eps)\n\n    fig, ax = plt.subplots()\n    line_color = ['dodgerblue', 'limegreen', 'red', 'mediumpurple', 'orange']\n    for idx, lmb in enumerate([3, 2, 1, 0, -1]):\n        y = power_func(x, lmb)\n        ax.plot(x, y, label=fr'$\\lambda$={lmb}', color=line_color[idx])\n    \n    ax.set_xlabel(r'$x$')\n    if power == \"BC\":\n        ax.set_ylabel(r'$\\psi_{\\text{BC}}(\\lambda, x)$')\n    else:\n        ax.set_ylabel(r'$\\psi_{\\text{YJ}}(\\lambda, x)$')\n\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(-4, 4)\n\n    ax.yaxis.set_ticks(list(np.arange(-4, 5, 2)))\n    ax.get_yticklabels()[2].set(color=\"red\")\n    ax.xaxis.set_ticks(list(np.arange(x_min, x_max + 1)))\n    if power == \"BC\":\n        ax.get_xticklabels()[1].set(color=\"red\")\n    else:\n        ax.get_xticklabels()[2].set(color=\"red\")\n\n    ax.axhline(0, linestyle='--', color='k')\n    if power == \"BC\":\n        ax.axvline(1, linestyle='--', color='k')\n    else:\n        ax.axvline(0, linestyle='--', color='k')\n\n    ax.grid()\n    ax.set_aspect(0.5)\n\n    leg = ax.legend(loc='lower right')\n    leg.get_texts()[2].set(color=\"red\")\n\n    plt.show()\n\n\n\nCode\npower_plot(x_min=0, x_max=3, power=\"BC\")\npower_plot(x_min=-2, x_max=2, power=\"YJ\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Box-Cox\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Yeo-Johnson\n\n\n\n\n\n\n\n\nFigure 1: Box-Cox and Yeo-Johnson transformations.\n\n\n\nThe optimal \\lambda is typically estimated by maximizing the log-likelihood. For Box-Cox and Yeo-Johnson, the respective log-likelihood functions are:\n\n\\ln\\mathcal{L}_{\\text{BC}}(\\lambda, x)=(\\lambda-1) \\sum_i^n \\ln x_i - \\frac{n}{2}\\ln\\sigma^2_{\\psi_{\\text{BC}}}\n\\tag{3}\n\n\\ln\\mathcal{L}_{\\text{YJ}}(\\lambda, x)=(\\lambda-1) \\sum_i^n \\text{sgn} (x_i) \\ln(|x_i|+1) - \\frac{n}{2}\\ln\\sigma^2_{\\psi_{\\text{YJ}}}\n\\tag{4}\nHere, \\sigma^2_\\psi represents the variance of the transformed data, \\text{Var}[\\psi(\\lambda,x)]. These log-likelihood functions are concave (Kouider and Chen 1995; Marchand et al. 2022), which guarantees a unique maximum. Brent’s method (Brent 2013) is commonly employed for optimization.\n\nfrom scipy.optimize import brent\n\ndef max_llf(x, llf):\n    def _neg_llf(lmb, x):\n        return -llf(lmb, x)\n    return brent(_neg_llf, args=(x,))"
  },
  {
    "objectID": "blog/power-transform.html#numerical-instabilities",
    "href": "blog/power-transform.html#numerical-instabilities",
    "title": "Power Transform",
    "section": "2 Numerical Instabilities",
    "text": "2 Numerical Instabilities\nBecause both transformations involve exponentiation, they are susceptible to numerical overflow. This problem has been observed by (Marchand et al. 2022) and discussed in Scikit-learn’s GitHub Issue. Below is an example illustrating the problem using a naive implementation of Equation 3:\n\ndef boxcox_llf_naive(lmb, x):\n    n = len(x)\n    logx = np.log(x)\n    logvar = np.log(np.var(boxcox(x, lmb)))\n    return (lmb - 1) * np.sum(logx) - n/2 * logvar\n\n\nx = np.array([10, 10, 10, 9.9])\nprint(max_llf(x, llf=boxcox_llf_naive))\n\n156.48528753755807\n\n\n/opt/miniconda3/lib/python3.12/site-packages/numpy/_core/_methods.py:194: RuntimeWarning:\n\noverflow encountered in multiply\n\n/opt/miniconda3/lib/python3.12/site-packages/numpy/_core/_methods.py:205: RuntimeWarning:\n\noverflow encountered in reduce\n\n\n\nAlthough this returns a \\lambda value, it produces overflow warnings. A useful diagnostic is to visualize the log-likelihood curve:\n\n\nCode\ndef plot_llf(x, lb, ub, llf):\n    np.set_printoptions(precision=3)\n    lmb = np.linspace(lb, ub, 20)\n    ll = np.array([llf(l, x) for l in lmb])\n    print(f\"llf={ll}\")\n\n    fig, ax = plt.subplots(figsize=(4, 4))\n    ax.plot(lmb, ll)\n    ax.set_xlabel(r\"$\\lambda$\")\n    ax.set_ylabel(r\"$\\ln\\mathcal{L}(\\lambda, x)$\")\n    plt.show()\n\n\n\nplot_llf(x, lb=150, ub=200, llf=boxcox_llf_naive)\n\nllf=[13.684 13.697 13.711   -inf   -inf   -inf   -inf   -inf   -inf   -inf\n   -inf   -inf   -inf   -inf   -inf   -inf   -inf   -inf   -inf   -inf]\n\n\n\n\n\n\n\n\n\nUnfortunately, due to overflow (np.inf values), the log-likelihood curve cannot be visualized in the specified range, suggesting the returned \\lambda is not optimal."
  },
  {
    "objectID": "blog/power-transform.html#existing-solutions",
    "href": "blog/power-transform.html#existing-solutions",
    "title": "Power Transform",
    "section": "3 Existing Solutions",
    "text": "3 Existing Solutions\nThe MASS package in R (Venables and Ripley 2002) proposes a simple yet effective trick: divide the data by its mean. This rescales the data and avoids numerical instability without affecting the optimization outcome.\n\nx_dm = x / np.mean(x)\nprint(max_llf(x_dm, llf=boxcox_llf_naive))\n\n357.55141884289054\n\n\n\nplot_llf(x_dm, lb=330, ub=385, llf=boxcox_llf_naive)\n\nllf=[23.376 23.377 23.378 23.38  23.381 23.381 23.382 23.383 23.383 23.383\n 23.383 23.383 23.383 23.382 23.382 23.381 23.38  23.379 23.377 23.376]\n\n\n\n\n\n\n\n\n\nTo see why this works, consider the log-variance term for \\lambda \\ne 0:\n\n\\begin{align*}\n\\ln\\text{Var}[\\psi_{\\text{BC}}(\\lambda,x)]\n&=\\ln\\text{Var}[(x^\\lambda-1)/\\lambda] \\\\\n&=\\ln\\text{Var}[x^\\lambda/\\lambda] \\\\\n&=\\ln[\\text{Var}(x^\\lambda)/\\lambda^2] \\\\\n&=\\ln\\text{Var}(x^\\lambda) - 2\\ln|\\lambda| \\\\\n\\end{align*}\n\\tag{5}\nIf x is scaled by a constant c &gt; 0:\n\n\\begin{align*}\n\\ln\\text{Var}[\\psi_{\\text{BC}}(\\lambda,x/c)]\n&=\\ln\\text{Var}[(x/c)^\\lambda] - 2\\ln|\\lambda| \\\\\n&=\\ln[\\text{Var}(x^\\lambda)/c^{2\\lambda}] - 2\\ln|\\lambda| \\\\\n&=\\ln\\text{Var}(x^\\lambda) - 2\\lambda\\ln c  - 2\\ln|\\lambda| \\\\\n&=\\ln\\text{Var}[\\psi_{\\text{BC}}(\\lambda,x)] - 2\\lambda\\ln c \\\\\n\\end{align*}\n\\tag{6}\nPlugging into the log−likelihood:\n\n\\begin{align*}\n\\ln\\mathcal{L}_{\\text{BC}}(\\lambda, x/c)\n&=(\\lambda-1) \\sum_i^n \\ln(x_i/c) - \\frac{n}{2}\\ln\\text{Var}[\\psi_{\\text{BC}}(\\lambda,x/c)] \\\\\n&=\\ln\\mathcal{L}_{\\text{BC}}(\\lambda, x) - n(\\lambda-1)\\ln c + n\\lambda\\ln c \\\\\n&=\\ln\\mathcal{L}_{\\text{BC}}(\\lambda, x) + n\\ln c \\\\\n\\end{align*}\n\\tag{7}\nThe additive constant n\\ln c does not affect the maximizer of \\lambda. However, this trick does not apply to Yeo-Johnson, where scaling alters the optimal parameter (see Equation 4)."
  },
  {
    "objectID": "blog/power-transform.html#log-space-computation",
    "href": "blog/power-transform.html#log-space-computation",
    "title": "Power Transform",
    "section": "4 Log-Space Computation",
    "text": "4 Log-Space Computation\nTo address numerical instability for both transformations, log-space computation (Haberland 2023) is effective. It uses the Log-Sum-Exp trick to compute statistics in log space:\n\nfrom scipy.special import logsumexp\n\ndef log_mean(logx):\n    # compute log of mean of x from log(x)\n    return logsumexp(logx) - np.log(len(logx))\n\ndef log_var(logx):\n    # compute log of variance of x from log(x)\n    logmean = log_mean(logx)\n    pij = np.full_like(logx, np.pi * 1j, dtype=np.complex128)\n    logxmu = logsumexp([logx, logmean + pij], axis=0)\n    return np.real(logsumexp(2 * logxmu)) - np.log(len(logx))\n\nThis allows direct computation of \\ln\\sigma^2_\\psi from \\ln x. Plugging in Equation 5, we can compute the log-likelihood in the log-space.\n\ndef boxcox_llf(lmb, x):\n    n = len(x)\n    logx = np.log(x)\n    if lmb == 0:\n        logvar = np.log(np.var(logx))\n    else:\n        logvar = log_var(lmb * logx) - 2 * np.log(abs(lmb))\n    return (lmb - 1) * np.sum(logx) - n/2 * logvar\n\nThis version avoids overflow and reliably returns the optimal \\lambda.\n\nprint(max_llf(x, llf=boxcox_llf))\n\n357.55141245531865\n\n\n\nplot_llf(x, lb=330, ub=385, llf=boxcox_llf)\n\nllf=[14.175 14.177 14.178 14.179 14.18  14.181 14.182 14.182 14.183 14.183\n 14.183 14.183 14.182 14.182 14.181 14.18  14.179 14.178 14.177 14.176]\n\n\n\n\n\n\n\n\n\nThe same principle extends to Yeo-Johnson, even for mixed-sign inputs.\n\n\nCode\ndef log_var_yeojohnson(x, lmb):\n    if np.all(x &gt;= 0):\n        if abs(lmb) &lt; np.spacing(1.0):\n            return np.log(np.var(np.log1p(x)))\n        return log_var(lmb * np.log1p(x)) - 2 * np.log(abs(lmb))\n\n    elif np.all(x &lt; 0):\n        if abs(lmb - 2) &lt; np.spacing(1.0):\n            return np.log(np.var(np.log1p(-x)))\n        return log_var((2 - lmb) * np.log1p(-x)) - 2 * np.log(abs(2 - lmb))\n\n    else:  # mixed positive and negtive data\n        logyj = np.zeros_like(x, dtype=np.complex128)\n        pos = x &gt;= 0\n\n        # when x &gt;= 0\n        if abs(lmb) &lt; np.spacing(1.0):\n            logyj[pos] = np.log(np.log1p(x[pos]) + 0j)\n        else:  # lmbda != 0\n            logm1_pos = np.full_like(x[pos], np.pi * 1j, dtype=np.complex128)\n            logyj[pos] = logsumexp(\n                [lmb * np.log1p(x[pos]), logm1_pos], axis=0\n            ) - np.log(lmb + 0j)\n\n        # when x &lt; 0\n        if abs(lmb - 2) &lt; np.spacing(1.0):\n            logyj[~pos] = np.log(-np.log1p(-x[~pos]) + 0j)\n        else:  # lmbda != 2\n            logm1_neg = np.full_like(x[~pos], np.pi * 1j, dtype=np.complex128)\n            logyj[~pos] = logsumexp(\n                [(2 - lmb) * np.log1p(-x[~pos]), logm1_neg], axis=0\n            ) - np.log(lmb - 2 + 0j)\n\n        return log_var(logyj)\n\ndef yeojohnson_llf(lmb, x):\n    n = len(x)\n    llf = (lmb - 1) * np.sum(np.sign(x) * np.log1p(np.abs(x)))\n    llf += -n / 2 * log_var_yeojohnson(x, lmb)\n    return llf"
  },
  {
    "objectID": "blog/power-transform.html#constrained-optimization",
    "href": "blog/power-transform.html#constrained-optimization",
    "title": "Power Transform",
    "section": "5 Constrained Optimization",
    "text": "5 Constrained Optimization\nEven with log-space computation, applying the transformation using a large absolute value of \\lambda can result in overflow:\n\nlmax = max_llf(x, llf=boxcox_llf)\nprint(boxcox(x, lmax))\n\n[inf inf inf inf]\n\n\nTo prevent this, we constrain \\lambda so that the transformed values stay within a safe range.\n\nLemma 1 The transformation \\psi(\\lambda,x) is monotonically increasing in both \\lambda and x (see I.-K. Yeo 1997 for Yeo-Johnson proof).\n\nThis allows us to bound \\lambda based on \\min(x) and \\max(x). For Box-Cox, we use x=1 as a threshold since \\psi_\\text{BC}(\\lambda,1)=0:\n\n\\begin{align*}\n\\max_\\lambda \\quad & \\ln\\mathcal{L}_{\\text{BC}}(\\lambda, x) \\\\\n\\textrm{s.t.} \\quad & \\text{if } x_{\\max}&gt;1, \\lambda\\le\\psi^{-1}_{\\text{BC}}(x_{\\max},y_{\\max}) \\\\\n& \\text{if } x_{\\min}&lt;1, \\lambda\\ge\\psi^{-1}_{\\text{BC}}(x_{\\min},-y_{\\max}) \\\\\n\\end{align*}\n\\tag{8}\nThe inverse Box−Cox function is given by:\n\n\\psi^{-1}_{\\text{BC}}(x,y)=-1/y-W(-x^{-1/y}\\ln x/y)/\\ln x\n\\tag{9}\n\nfrom scipy.special import lambertw\n\ndef boxcox_inv_lmbda(x, y):\n    num = lambertw(-(x ** (-1 / y)) * np.log(x) / y, k=-1)\n    return -1 / y - np.real(num / np.log(x))\n\n\ndef boxcox_constranined_lmax(lmax, x, ymax):\n    # x &gt; 1, boxcox(x) &gt; 0; x &lt; 1, boxcox(x) &lt; 0\n    xmin, xmax = min(x), max(x)\n    if xmin &gt;= 1:\n        x_treme = xmax\n    elif xmax &lt;= 1:\n        x_treme = xmin\n    else:  # xmin &lt; 1 &lt; xmax\n        indicator = boxcox(xmax, lmax) &gt; abs(boxcox(xmin, lmax))\n        x_treme = xmax if indicator else xmin\n    \n    if abs(boxcox(x_treme, lmax)) &gt; ymax:\n        lmax = boxcox_inv_lmbda(x_treme, ymax * np.sign(x_treme - 1))\n    return lmax\n\nThis method can be verified by testing different values for y_{\\max}:\n\ndef verify_boxcox_constranined_lmax(x):\n    np.set_printoptions(precision=3)\n    lmax = max_llf(x, llf=boxcox_llf)\n    for ymax in [1e300, 1e100, 1e30, 1e10]:\n        l = boxcox_constranined_lmax(lmax, x, ymax)\n        print(boxcox(x, l))\n\n\n# Positive overflow\nx = np.array([10, 10, 10, 9.9])\nverify_boxcox_constranined_lmax(x)\n\n[1.000e+300 1.000e+300 1.000e+300 4.783e+298]\n[1.000e+100 1.000e+100 1.000e+100 3.587e+099]\n[1.000e+30 1.000e+30 1.000e+30 7.286e+29]\n[1.00e+10 1.00e+10 1.00e+10 8.95e+09]\n\n\n\n# Negative overflow\nx = np.array([0.1, 0.1, 0.1, 0.101])\nverify_boxcox_constranined_lmax(x)\n\n[-1.00e+300 -1.00e+300 -1.00e+300 -4.93e+298]\n[-1.000e+100 -1.000e+100 -1.000e+100 -3.624e+099]\n[-1.000e+30 -1.000e+30 -1.000e+30 -7.309e+29]\n[-1.000e+10 -1.000e+10 -1.000e+10 -8.959e+09]\n\n\nThe constrained optimization approach can also be extended to Yeo-Johnson, ensures overflow-free transformations even for extreme values.\n\n\nCode\ndef yeojohnson_inv_lmbda(x, y):\n    if x &gt;= 0:\n        num = lambertw(-((x + 1) ** (-1 / y) * np.log1p(x)) / y, k=-1)\n        return -1 / y + np.real(-num / np.log1p(x))\n    else:\n        num = lambertw(((1 - x) ** (1 / y) * np.log1p(-x)) / y, k=-1)\n        return -1 / y + 2 + np.real(num / np.log1p(-x))\n\ndef yeojohnson_constranined_lmax(lmax, x, ymax):\n    # x &gt; 0, yeojohnson(x) &gt; 0; x &lt; 0, yeojohnson(x) &lt; 0\n    xmin, xmax = min(x), max(x)\n    if xmin &gt;= 0:\n        x_treme = xmax\n    elif xmax &lt;= 0:\n        x_treme = xmin\n    else:  # xmin &lt; 0 &lt; xmax\n        with np.errstate(over=\"ignore\"):\n            indicator = yeojohnson(xmax, lmax) &gt; abs(yeojohnson(xmin, lmax))\n        x_treme = xmax if indicator else xmin\n\n    with np.errstate(over=\"ignore\"):\n        if abs(yeojohnson(x_treme, lmax)) &gt; ymax:\n            lmax = yeojohnson_inv_lmbda(x_treme, ymax * np.sign(x_treme))\n    return lmax"
  },
  {
    "objectID": "teach/index.html",
    "href": "teach/index.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "@ University of Warwick\n\nCS429/CS909 Data Mining: Spring 2025\nCS133 Professional Skills: Fall 2024\n\n\n\n@ Beihang University\n\n39112201 Abstract Algebra: Fall 2019"
  }
]