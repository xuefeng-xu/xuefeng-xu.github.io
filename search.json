[
  {
    "objectID": "teach/teach.html",
    "href": "teach/teach.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "Teaching Assistant\n\n@ University of Warwick\n\nCS275 Probability and Statistics: Autumn 2025\nCS429/CS909 Data Mining: Spring 2025\nCS133 Professional Skills: Autumn 2024\n\n\n\n@ Beihang University\n\n39112201 Abstract Algebra: Autumn 2019"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "University of Warwick | Coventry, UK Ph.D. in Computer Science | September 2024 - Present\n\n\nBeihang University | Beijing, China M.E. in Cyberspace Security | September 2019 - January 2022\n\n\nJilin University | Changchun, China B.E. in Communication Engineering | September 2015 - July 2019"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "University of Warwick | Coventry, UK Ph.D. in Computer Science | September 2024 - Present\n\n\nBeihang University | Beijing, China M.E. in Cyberspace Security | September 2019 - January 2022\n\n\nJilin University | Changchun, China B.E. in Communication Engineering | September 2015 - July 2019"
  },
  {
    "objectID": "cv.html#experience",
    "href": "cv.html#experience",
    "title": "Xuefeng Xu",
    "section": "Experience",
    "text": "Experience\n\n\nPrimiHub | Beijing, China Privacy Engineer | December 2022 - August 2024\n\n\nOPPO | Shenzhen, China Machine Learning Intern | May 2022 - August 2022"
  },
  {
    "objectID": "cv.html#research",
    "href": "cv.html#research",
    "title": "Xuefeng Xu",
    "section": "Research",
    "text": "Research\n\n\nPower Transform Revisited: Numerically Stable, and Federated Xuefeng Xu and Graham Cormode In Submission\n\n\nFederated Computation of ROC and PR Curves Xuefeng Xu and Graham Cormode In Submission\n\n\nFedPS: Federated data Preprocessing via aggregated Statistics Code\n\n\nDeep Learning Algorithms Design and Implementation Based on Differential Privacy Xuefeng Xu, Yanqing Yao, and Lei Cheng International Conference on Machine Learning for Cyber Security, 2020"
  },
  {
    "objectID": "cv.html#teaching",
    "href": "cv.html#teaching",
    "title": "Xuefeng Xu",
    "section": "Teaching",
    "text": "Teaching\n\nTeaching Assistant\n\n@ University of Warwick\n\nCS275 Probability and Statistics: Autumn 2025\nCS429/CS909 Data Mining: Spring 2025\nCS133 Professional Skills: Autumn 2024\n\n\n\n@ Beihang University\n\n39112201 Abstract Algebra: Autumn 2019"
  },
  {
    "objectID": "cv.html#service",
    "href": "cv.html#service",
    "title": "Xuefeng Xu",
    "section": "Service",
    "text": "Service\n\nConference Reviewer\n\nAnnual Conference on Artificial Intelligence and Statistics (AISTATS): 2026\nInternational Conference on Learning Representations (ICLR): 2025, 2026"
  },
  {
    "objectID": "blog/roc-pr-curve.html",
    "href": "blog/roc-pr-curve.html",
    "title": "ROC and PR Curves",
    "section": "",
    "text": "Receiver Operating Characteristic (ROC) and Precision–Recall (PR) curves are tools for evaluating classification models. They visualize trade-offs between True Positive Rate (TPR) and False Positive Rate (FPR) (ROC) or Precision and Recall (PR), offering deeper insights than single scalar metrics."
  },
  {
    "objectID": "blog/roc-pr-curve.html#roc-curve",
    "href": "blog/roc-pr-curve.html#roc-curve",
    "title": "ROC and PR Curves",
    "section": "1 ROC Curve",
    "text": "1 ROC Curve\nFor binary classification, with Positive (P) and Negative (N) classes, predictions are based on model scores thresholded to produce labels. Performance is summarized using a confusion matrix (Table 1), counting True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN).\n\n\n\nTable 1: The Confusion Matrix\n\n\n\n\n\nTotal=P+N\nPredicted Pos (PP)\nPredicted Neg (PN)\n\n\n\n\n\nActual Pos (P)\nTP\nFN\nP=TP+FN\n\n\nActual Neg (N)\nFP\nTN\nN=FP+TN\n\n\n\nPP=TP+FP\nPN=FN+TN\n\n\n\n\n\n\n\nThe ROC curve plots TPR vs. FPR, where:\n\n\\text{TPR}=\\text{TP}/\\text{P},\\quad \\text{FPR}=\\text{FP}/\\text{N}\n\\tag{1}\nA simple algorithm constructs the ROC curve: (i) Sort predictions in descending score order. (ii) Sweep thresholds across scores, updating TP and FP counts (Fawcett 2006).\n\nimport numpy as np\n\ndef count_fp_tp(y_true, y_score):\n    fp, tp = 0, 0\n    fps, tps, thresholds = [], [], []\n    score = np.inf\n\n    for i in np.flip(np.argsort(y_score)):\n1        if y_score[i] != score:\n            fps.append(fp)\n            tps.append(tp)\n            thresholds.append(score)\n            score = y_score[i]\n\n        if y_true[i] == 1:\n            tp += 1\n        else:\n            fp += 1\n\n    fps.append(fp)\n    tps.append(tp)\n    thresholds.append(score)\n    return np.asarray(fps), np.asarray(tps), np.asarray(thresholds)\n\n\n1\n\nTo handle score repetitions, record (FP, TP) counts only when the score changes.\n\n\n\n\n\ndef roc_curve(y_true, y_score):\n    fps, tps, thresholds = count_fp_tp(y_true, y_score)\n    fpr = fps / sum(y_true == 0)\n    tpr = tps / sum(y_true == 1)\n    return fpr, tpr, thresholds\n\nThe ROC curve is always monotonically non-decreasing, starting at (0, 0) for the highest threshold and ending at (1, 1) for the lowest threshold. Points are connected using linear interpolation. A random classifier lies along y=x, while curves closer to the top-left indicate better performance.\n\n\nCode\nimport matplotlib.pyplot as plt\n\ndef plot_curve(x, y, curve: str, pos_frac=None, drawstyle=\"default\", ax=None):\n    if ax is None:\n        _, ax = plt.subplots(figsize=(3, 3))\n\n    name = f\"{curve} curve\"\n    ax.plot(x, y, \"x-\", c=\"C0\", label=name, drawstyle=drawstyle)\n    ax.set_title(name)\n\n    ax.set_aspect(\"equal\")\n    ax.set_xlim(-0.01, 1.01)\n    ax.set_ylim(-0.01, 1.01)\n    \n    chance = {\n        \"label\": \"Chance level\",\n        \"color\": \"k\",\n        \"linestyle\": \"--\",\n    }\n    if curve == \"ROC\":\n        ax.plot([0, 1], [0, 1], **chance)\n        ax.set_xlabel(\"FPR\")\n        ax.set_ylabel(\"TPR\")\n        ax.legend(loc=\"lower right\")\n    elif curve == \"PR\":  # curve == \"PR\":\n        if pos_frac is not None:\n            ax.plot([0, 1], [pos_frac, pos_frac], **chance)\n        ax.set_xlabel(\"Recall\")\n        ax.set_ylabel(\"Precision\")\n        ax.legend(loc=\"lower left\")\n    elif curve == \"PRG\":\n        ax.set_xlabel(\"Recall Gain\")\n        ax.set_ylabel(\"Precision Gain\")\n        ax.legend(loc=\"lower left\")\n    else:\n        raise ValueError(f\"Unknown curve type: {curve}\")\n\n\n\ny_true = np.array([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0])\ny_score = np.array([\n    .9, .8, .7, .6, .55, .54, .53, .52, .51, .505,\n    .4, .39, .38, .37, .36, .35, .34, .33, .3, .1\n])\n\nfpr, tpr, _ = roc_curve(y_true, y_score)\nplot_curve(fpr, tpr, \"ROC\")\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "blog/roc-pr-curve.html#roc-convex-hull",
    "href": "blog/roc-pr-curve.html#roc-convex-hull",
    "title": "ROC and PR Curves",
    "section": "2 ROC Convex Hull",
    "text": "2 ROC Convex Hull\nThe ROC Convex Hull (ROCCH) highlights potentially optimal performance of a classifier (or a set of classifiers) by connecting upper boundary points (Provost and Fawcett 2001). It can be computed via algorithms like the Monotone chain algorithm.\n\ndef convex_hull(points):\n    points = sorted(set(points))\n\n    def cross(o, a, b):\n        return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0])\n\n    # Build upper hull\n    upper = []\n    for p in reversed(points):\n        while len(upper) &gt;= 2 and cross(upper[-2], upper[-1], p) &lt;= 0:\n            upper.pop()\n        upper.append(p)\n    \n    return upper\n\n\n\nrocch = convex_hull(zip(fpr, tpr))\n\nrocch = np.array(rocch)\nfpr_ch, tpr_ch = rocch[:, 0], rocch[:, 1]\n\nfig, ax = plt.subplots(figsize=(3, 3))\nax.plot(fpr_ch, tpr_ch, 'C1-.', label=\"ROCCH\")\nplot_curve(fpr, tpr, \"ROC\", ax=ax)\n\n\n\n\n\n\n\nFigure 2"
  },
  {
    "objectID": "blog/roc-pr-curve.html#pr-curve",
    "href": "blog/roc-pr-curve.html#pr-curve",
    "title": "ROC and PR Curves",
    "section": "3 PR Curve",
    "text": "3 PR Curve\nPR curves plot Precision vs. Recall. Recall equals TPR, while Precision is:\n\n\\text{Prec}=\\frac{\\text{TP}}{\\text{TP}+\\text{FP}}\n\\tag{2}\nPR curves start at (0, 1) and end at (1, \\frac{\\text{P}}{\\text{P}+\\text{N}}), where the endpoint reflects the proportion of positive samples. Because Precision depends on both TP and FP, the PR curve is non-monotonic and can decrease as Recall increases. Moreover, it is highly influenced by class imbalance (Williams 2021). For a random classifier, the PR curve is a horizontal line at y = \\frac{\\text{P}}{\\text{P}+\\text{N}}, whereas curves closer to the top-right indicate stronger performance.\n\ndef pr_curve(y_true, y_score):\n    fps, tps, thresholds = count_fp_tp(y_true, y_score)\n\n    pps = fps + tps\n    precision = np.ones_like(tps, dtype=np.float64)\n    np.divide(tps, pps, out=precision, where=(pps != 0))\n\n    recall = tps / sum(y_true == 1)\n    return np.flip(precision), np.flip(recall), np.flip(thresholds)\n\nAlthough ROC and PR curves are mathematically related, linear interpolation is incorrect for PR curves, as it yields overly optimistic estimates of performance (Davis and Goadrich 2006). Precision does not necessarily vary linearly with Recall, so naive interpolation misrepresents true model behavior.\nTo illustrate this, we convert the ROC Convex Hull (ROCCH) into PR space, treating it as the potential optimal PR curve. Using Equation 3, we express Precision in terms of TPR and FPR:\n\n\\text{Prec}=\\frac{\\text{P}\\cdot\\text{TPR}}{\\text{P}\\cdot\\text{TPR}+\\text{N}\\cdot\\text{FPR}}\n=\\frac{1}{1+\\frac{\\text{N}}{\\text{P}}\\cdot\\frac{\\text{FPR}}{\\text{TPR}}}\n\\tag{3}\n\nfrom scipy.interpolate import interp1d\n\ndef pr_from_roc(fpr, tpr, neg_to_pos):\n    fpr_tpr_func = interp1d(tpr, fpr, bounds_error=False, fill_value=(0, 1))\n\n    def prec_func(tpr):\n        fpr = fpr_tpr_func(tpr)\n        fpr_to_tpr = np.zeros_like(tpr, dtype=np.float64)\n        np.divide(fpr, tpr, out=fpr_to_tpr, where=(tpr != 0))\n        prec = 1 / (1 + neg_to_pos * fpr_to_tpr)\n        return prec\n\n    return prec_func\n\n\nneg_to_pos = sum(y_true == 0) / sum(y_true == 1)\nprec_ch_func = pr_from_roc(fpr_ch, tpr_ch, neg_to_pos)\n\nfig, ax = plt.subplots(figsize=(3, 3))\nax.plot(tpr_ch, prec_ch_func(tpr_ch), 'C2:', label=\"Linear interp ROCCH\")\n\nx = np.linspace(0, 1, 100)\nax.plot(x, prec_ch_func(x), 'C1-.', label=\"ROCCH\")\n\nprecision, recall, _ = pr_curve(y_true, y_score)\nax.scatter(recall, precision, c=\"C0\", marker=\"x\", label=\"PR points\")\n\nax.set_aspect(\"equal\")\nax.set_xlim(-0.01, 1.01)\nax.set_ylim(-0.01, 1.01)\nax.set_xlabel(\"Recall\")\nax.set_ylabel(\"Precision\")\nax.legend(loc=\"lower left\")\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nThe converted ROCCH dominates the PR space but is clearly non-linear, lying beneath its own linear interpolation (dotted line). This confirms that PR curves cannot be linearly interpolated. Scikit-learn’s PrecisionRecallDisplay instead uses step-wise interpolation, consistent with average_precision_score. This ensures that the area under the PR curve equals the average precision.\n\npos_frac = sum(y_true == 1) / len(y_true)\nplot_curve(recall, precision, \"PR\", pos_frac, drawstyle=\"steps-post\")\n\n\n\n\n\n\n\nFigure 4"
  },
  {
    "objectID": "blog/roc-pr-curve.html#prg-curve",
    "href": "blog/roc-pr-curve.html#prg-curve",
    "title": "ROC and PR Curves",
    "section": "4 PRG Curve",
    "text": "4 PRG Curve\nROC curves benefit from linear interpolation and universal baselines, whereas PR curves lack these properties. To address this, Flach and Kull (2015) proposed the Precision–Recall-Gain (PRG) curve, which transforms Precision and Recall into Gains and plots these Precision Gain and Recall Gain within the unit square:\n\n\\text{PrecG}=\\frac{\\text{Prec}-\\pi}{(1-\\pi)\\text{Prec}},\\quad\n\\text{RecG}=\\frac{\\text{Rec}-\\pi}{(1-\\pi)\\text{Rec}}\n\\tag{4}\nwhere \\pi=\\frac{\\text{P}}{\\text{P}+\\text{N}} is the positive class fraction. Notably, Gains can be negative, and such points are typically omitted from the PRG curve.\n\ndef prg_curve(y_true, y_score):\n    prec, rec, thresholds = pr_curve(y_true, y_score)\n    pos_frac = sum(y_true == 0) / len(y_true)\n\n    # Remove negative gains\n    mask = (rec &gt;= pos_frac) & (prec &gt;= pos_frac)\n    prec = prec[mask]\n    rec = rec[mask]\n\n    prec_gain = (prec - pos_frac) / (1 - pos_frac) / prec\n    rec_gain = (rec - pos_frac) / (1 - pos_frac) / rec\n    return prec_gain, rec_gain, thresholds\n\n\nprec_gain, rec_gain, _ = prg_curve(y_true, y_score)\nplot_curve(rec_gain, prec_gain, \"PRG\")\n\n\n\n\n\n\n\nFigure 5"
  },
  {
    "objectID": "blog/variance.html",
    "href": "blog/variance.html",
    "title": "How to Compute Variance?",
    "section": "",
    "text": "Given a dataset \\{x_1, \\ldots, x_n\\}, variance measures how much data points deviate from their mean. While the mathematical definition is straightforward, the way we implement variance can have a big impact on numerical stability."
  },
  {
    "objectID": "blog/variance.html#textbook-formulas",
    "href": "blog/variance.html#textbook-formulas",
    "title": "How to Compute Variance?",
    "section": "1 Textbook Formulas",
    "text": "1 Textbook Formulas\nThe most common definition computes the mean first, then the average squared deviation from it:\n\n\\mu = \\frac{1}{n} \\sum_{i=1}^n x_i,\\ \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2.\n\\tag{1}\n\ndef var_twopass_textbook(x):\n    n, s = 0, 0\n    for xi in x:\n        n += 1\n        s += xi\n\n    mu = s / n\n    sd = 0\n    for xi in x:\n        sd += (xi - mu) ** 2\n    return sd / n\n\nThis requires two passes over the data. For streaming or memory-constrained scenarios, a one-pass formula is often considered:\n\n\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n x_i^2 - \\left(\\frac{1}{n} \\sum_{i=1}^n x_i\\right)^2.\n\\tag{2}\n\ndef var_onepass_textbook(x):\n    n, s, sq = 0, 0, 0\n    for xi in x:\n        n += 1\n        s += xi\n        sq += xi ** 2\n    return sq / n - (s / n) ** 2\n\nIn exact arithmetic, these two formulas are identical. But in floating-point arithmetic, they can yield very different results."
  },
  {
    "objectID": "blog/variance.html#numerical-cancellation",
    "href": "blog/variance.html#numerical-cancellation",
    "title": "How to Compute Variance?",
    "section": "2 Numerical Cancellation",
    "text": "2 Numerical Cancellation\nLet’s test both formulas on a simple dataset:\n\nimport numpy as np\n\nx = np.array([0., 1., 2.])\nprint(\"Two-pass (Textbook):\", var_twopass_textbook(x))\nprint(\"One-pass (Textbook):\", var_onepass_textbook(x))\n\nTwo-pass (Textbook): 0.6666666666666666\nOne-pass (Textbook): 0.6666666666666667\n\n\nSo far, so good. But if we shift all values by a large constant, the one-pass version quickly breaks down due to cancellation errors:\n\nfor shift in [1e3, 1e6, 1e9, 1e12]:\n    xt = x + shift\n    print(f\"Shift: {shift:.0e}\")\n    print(f\"Two-pass (Textbook): {var_twopass_textbook(xt)}\")\n    print(f\"One-pass (Textbook): {var_onepass_textbook(xt)}\\n\")\n\nShift: 1e+03\nTwo-pass (Textbook): 0.6666666666666666\nOne-pass (Textbook): 0.6666666666278616\n\nShift: 1e+06\nTwo-pass (Textbook): 0.6666666666666666\nOne-pass (Textbook): 0.6666259765625\n\nShift: 1e+09\nTwo-pass (Textbook): 0.6666666666666666\nOne-pass (Textbook): 0.0\n\nShift: 1e+12\nTwo-pass (Textbook): 0.6666666666666666\nOne-pass (Textbook): -134217728.0\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\ndef plot_errors(x, var_func, error_type=\"absolute\"):\n    ref = np.var(x)\n    shifts = [10**i for i in range(0, 20)]\n\n    errors = []\n    for shift in shifts:\n        xt = x + shift\n        var = var_func(xt)\n\n        if error_type == \"absolute\":\n            errors.append(abs(var - ref))\n        elif error_type == \"relative\":\n            errors.append(abs(var - ref) / abs(ref))\n\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax.loglog(shifts, errors, \"*-r\")\n    ax.set_xlabel(\"Shift value\")\n    ax.set_ylabel(f\"{error_type.capitalize()} error\")\n    ax.set_title(f\"{var_func.__name__}\")\n    ax.grid()\n\n\nplot_errors(x, var_onepass_textbook, error_type=\"absolute\")\nplot_errors(x, var_onepass_textbook, error_type=\"relative\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Absolute error\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Relative error\n\n\n\n\n\n\n\n\n\nFigure 1: Error plots for the one-pass textbook formula.\n\n\n\nThis instability makes the textbook one-pass formula unsuitable for real-world use."
  },
  {
    "objectID": "blog/variance.html#floating-point-arithmetic",
    "href": "blog/variance.html#floating-point-arithmetic",
    "title": "How to Compute Variance?",
    "section": "3 Floating Point Arithmetic",
    "text": "3 Floating Point Arithmetic\nWhy does this happen? Because floating-point numbers can only represent a finite number of digits. Squaring a large number, for example, can lose precision:\n\n1000000001.0 ** 2\n# 1.000000002000000001e+18\n\n1.000000002e+18\n\n\nMoreover, floating point numbers cannot exactly represent all values, especially very large numbers, leading to rounding errors.\n\n1000000002000000001.0\n\n1.000000002e+18\n\n\nMost systems use the IEEE 754 standard. In double precision (64-bit), numbers have 53 bits of precision, so integers larger than 2^{53}-1 (9007199254740991) cannot be represented exactly. For example, adding small increments beyond this threshold reveals precision loss:\n\nprint(9007199254740991.0)        # correct\nprint(9007199254740991.0 + 1.0)  # still correct\nprint(9007199254740991.0 + 2.0)  # wrong\n\n9007199254740991.0\n9007199254740992.0\n9007199254740992.0\n\n\nThe hex representation makes this clear:\n\nprint(hex(9007199254740991))  # exact\nprint(hex(9007199254740992))  # exact\nprint(hex(9007199254740993))  # last bit lost\n\n0x1fffffffffffff\n0x20000000000000\n0x20000000000001\n\n\nLarge numbers like:\n\nprint(hex(1000000002000000001))\n\n0xde0b6b41e999401\n\n\nalso show lost lower bits, which is exactly what causes variance miscalculations."
  },
  {
    "objectID": "blog/variance.html#stable-alternatives",
    "href": "blog/variance.html#stable-alternatives",
    "title": "How to Compute Variance?",
    "section": "4 Stable Alternatives",
    "text": "4 Stable Alternatives\nA better one-pass approach is Welford’s algorithm (Welford 1962). Instead of subtracting large similar terms, it incrementally updates the mean \\mu_k=\\sum_{i=1}^k x_i / k and the squared deviations S_k=\\sum_{i=1}^k (x_i - \\mu_k)^2 to maintain numerical stability:\n\n\\begin{align*}\n\\delta_k &= x_k - \\mu_{k-1},\\\\\n\\mu_k &= \\mu_{k-1} + \\delta_k / k,\\\\\nS_k &= S_{k-1} + \\delta_k (x_k - \\mu_k).\n\\end{align*}\n\\tag{3}\n\ndef var_onepass_welford(x):\n    n, mu, sd = 0, 0, 0\n    for xi in x:\n        n += 1\n        delta = xi - mu\n        mu += delta / n\n        sd += delta * (xi - mu)\n    return sd / n\n\nTesting again with shifts:\n\nfor shift in [1e3, 1e6, 1e9, 1e12]:\n    xt = x + shift\n    print(f\"Shift: {shift:.0e}\")\n    print(f\"One-pass (Welford): {var_onepass_welford(xt)}\\n\")\n\nShift: 1e+03\nOne-pass (Welford): 0.6666666666666666\n\nShift: 1e+06\nOne-pass (Welford): 0.6666666666666666\n\nShift: 1e+09\nOne-pass (Welford): 0.6666666666666666\n\nShift: 1e+12\nOne-pass (Welford): 0.6666666666666666\n\n\n\nWelford’s method remains stable even under extreme shifts. Other methods, such as Chan’s algorithm (T. F. Chan, Golub, and LeVeque 1982), are also numerically stable. Classic studies (Ling 1974; Tony F. Chan, Golub, and Leveque 1983) provide detailed comparisons."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Xuefeng Xu 许雪峰",
    "section": "",
    "text": "Email: xuefeng.xu@warwick.ac.uk\n \n  \n   \n  \n    \n     GitHub\n  \n  \n      Google Scholar\n  \n\n\n\n\nBiography\nI am a Ph.D. student in Computer Science at the University of Warwick, supervised by Graham Cormode and collaborating with Hakan Ferhatosmanoglu. My research interests are privacy-preserving algorithms, particularly in the context of privacy-preserving machine learning. I received my M.E. from Beihang University and B.E. from Jilin University.\n\n\nNews\n\n09/2025: Volunteer at VLDB Conference 2025\n08/2025: Poster presentation at CISPA - ELLIS Summer School 2025\n03/2025: Online talk at Flower AI Summit 2025\n02/2025: Lightning talk at Privacy in Machine Learning Meetup"
  },
  {
    "objectID": "misc/resource.html",
    "href": "misc/resource.html",
    "title": "Study Resources",
    "section": "",
    "text": "Books\n\nIntroduction to Linear Algebra\nIntroduction to Probability for Computing\nAlgorithms\nNumerical Recipes: The Art of Scientific Computing\nPattern Recognition and Machine Learning\nThe Elements of Statistical Learning\nProbabilistic Machine Learning\nFoundations of Machine Learning\nThe Algorithmic Foundations of Differential Privacy\nThe Complexity of Differential Privacy\nIntroduction to Modern Cryptography\nEffective Python\n\n\n\nCourses\n\nMIT 18.06 Linear Algebra\nCoursera Algorithms Part 1 and Part 2\nStanford CS229 Machine Learning\nCoursera Deep Learning Specialization\nCoursera Cryptography\nMIT The Missing Semester of Your CS Education"
  },
  {
    "objectID": "research/powertf/index.html",
    "href": "research/powertf/index.html",
    "title": "Power Transform Revisited: Numerically Stable, and Federated",
    "section": "",
    "text": "Paper\n  \n  \n    \n     Code"
  },
  {
    "objectID": "research/powertf/index.html#introduction",
    "href": "research/powertf/index.html#introduction",
    "title": "Power Transform Revisited: Numerically Stable, and Federated",
    "section": "Introduction",
    "text": "Introduction\nPower transforms are widely used parametric methods in statistics and machine learning to make data more Gaussian-like. However, standard implementations suffer from numerical instabilities that can cause incorrect parameter estimates or even runtime failures.\nWe provide a systematic analysis of these issues and propose effective remedies. We further extend our approach to the federated learning setting, tackling both numerical stability and distributed computation. Finally, we validate our methods on real-world datasets."
  },
  {
    "objectID": "research/powertf/index.html#challenge-1-catastrophic-cancellation",
    "href": "research/powertf/index.html#challenge-1-catastrophic-cancellation",
    "title": "Power Transform Revisited: Numerically Stable, and Federated",
    "section": "Challenge 1: Catastrophic Cancellation",
    "text": "Challenge 1: Catastrophic Cancellation\nTwo common transforms are Box–Cox and Yeo–Johnson. For Box–Cox, with parameter \\lambda, the transform is:\n\n\\psi(\\lambda, x) =\n\\begin{cases}\n\\frac{x^\\lambda-1}{\\lambda} & \\text{if } \\lambda\\neq0,\\\\\n\\ln x & \\text{if } \\lambda=0.\n\\end{cases}\n\\tag{1}\nGiven data X = \\{x_1,\\dots,x_n\\}, the optimal \\lambda^* minimizes the negative log-likelihood (NLL):\n\n\\text{NLL} = (1-\\lambda) \\sum_{i=1}^n \\ln x_i\n+ \\frac{n}{2} \\ln \\text{Var}[\\psi(\\lambda, x)].\n\\tag{2}\nThe variance term is especially prone to cancellation. Consider three mathematically equivalent forms:\n\nKeep the constant term: \n\\ln \\text{Var}[\\psi(\\lambda, x)] = \\ln \\text{Var}[(x^\\lambda - 1) / \\lambda]\n\\tag{3}\nRemove the constant term: \n\\ln \\text{Var}[\\psi(\\lambda, x)] = \\ln \\text{Var}(x^\\lambda / \\lambda)\n\\tag{4}\nFactor out \\lambda: \n\\ln \\text{Var}[\\psi(\\lambda, x)] = \\ln \\text{Var}(x^\\lambda) - 2\\ln |\\lambda|\n\\tag{5}\n\nAs shown in Figure 1, different forms yield very different numerical behavior. Removing the constant term and factoring out \\lambda produces smooth NLL curves, while the other forms lead to spikes and discontinuities.\n\n\n\n\n\n\n\n\nEquation 3 vs. Equation 4\n\n\n\n\n\n\n\nEquation 4 vs. Equation 5\n\n\n\n\n\n\nFigure 1: NLL computed under different variance formulas.\n\n\n\n\nTakeaway 1: Equivalent formulas can behave differently, careful analysis is essential."
  },
  {
    "objectID": "research/powertf/index.html#challenge-2-numerical-overflow",
    "href": "research/powertf/index.html#challenge-2-numerical-overflow",
    "title": "Power Transform Revisited: Numerically Stable, and Federated",
    "section": "Challenge 2: Numerical Overflow",
    "text": "Challenge 2: Numerical Overflow\nVariance terms are also prone to overflow. We construct adversarial datasets that trigger overflow across floating-point precisions for Box-Cox:\n\n\n\nTable 1: Adversarial datasets inducing overflow\n\n\n\n\n\n\n\n\n\n\n\n\nOverflow\nAdversarial data\n\\lambda^*\nExtreme value\nMax Value (Precision)\n\n\n\n\nNegative\n[0.1, 0.1, 0.1, 0.101]\n-361.15\n-3.87e+358\n1.80e+308 (Double)\n\n\nPositive\n[10, 10, 10, 9.9]\n357.55\n9.96e+354\n\n\nNegative\n[0.1, 0.1, 0.1, 0.10001]\n-35936.9\n-2.30e+35932\n1.19e+4932 (Quadruple)\n\n\nPositive\n[10, 10, 10, 9.999]\n35933.3\n5.85e+35928\n\n\nNegative\n[0.1, 0.1, 0.1, 0.100001]\n-359353.0\n-2.74e+359347\n1.61e+78913 (Octuple)\n\n\nPositive\n[10, 10, 10, 9.9999]\n359349.0\n6.99e+359343\n\n\n\n\n\n\nSuch issues also appear in real-world dataset. Figure 2 shows the histograms of skewed and large-value features in the Ecoli and House datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Histograms of skewed and large-valued features.\n\n\n\nTo avoid overflow, we use log-domain computation for the variance. Figure 3 compares NLL curves: linear-domain breaks down, while log-domain remains continuous and stable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: NLL computed in log-domain vs. linear-domain.\n\n\n\n\nTakeaway 2: Use log-domain computation when handling extreme values."
  },
  {
    "objectID": "research/powertf/index.html#challenge-3-federated-aggregation",
    "href": "research/powertf/index.html#challenge-3-federated-aggregation",
    "title": "Power Transform Revisited: Numerically Stable, and Federated",
    "section": "Challenge 3: Federated Aggregation",
    "text": "Challenge 3: Federated Aggregation\nIn federated learning, we want to estimate a global \\lambda^* across distributed clients. This requires stable computation of global NLL under communication constraints.\nA naive one-pass method has clients send (n, \\sum x, \\sum x^2), and the server computes:\n\n\\frac{1}{n} \\sum_{i=1}^n x_i^2 - \\frac{1}{n^2} \\left(\\sum_{i=1}^n x_i\\right)^2\n\\tag{6}\nThe Equation 6 is unstable. Instead, we adopt the pairwise algorithm: clients send (n, \\bar{x}, s) with mean \\bar{x} and sum of squared deviations s=\\sum(x-\\bar{x})^2. The server merges them in a tree structure (Figure 4):\n\n\n\n\n\n\nFigure 4: Tree-style pairwise aggregation.\n\n\n\nThe merging formulas are:\n\nn^{(AB)} = n^{(A)} + n^{(B)},\n\\tag{7} \n\\bar{x}^{(AB)} = \\bar{x}^{(A)} + (\\bar{x}^{(B)} - \\bar{x}^{(A)}) \\cdot \\frac{n^{(B)}}{n^{(AB)}},\n\\tag{8} \ns^{(AB)} = s^{(A)} + s^{(B)} +(\\bar{x}^{(B)} - \\bar{x}^{(A)})^2 \\cdot \\frac{n^{(A)}n^{(B)}}{n^{(AB)}}.\n\\tag{9}\nFigure 5 shows that naive formulas produce spiky curves, while pairwise aggregation yields smooth ones.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: NLL in federated setting: naive vs. pairwise.\n\n\n\n\nTakeaway 3: Avoid textbook one-pass variance formula in numerical computing."
  },
  {
    "objectID": "research/powertf/index.html#open-source-contributions",
    "href": "research/powertf/index.html#open-source-contributions",
    "title": "Power Transform Revisited: Numerically Stable, and Federated",
    "section": "Open Source Contributions",
    "text": "Open Source Contributions\nOur methods have been integrated into SciPy and Scikit-learn:\n\nscipy.stats.boxcox_llf\nscipy.stats.boxcox_normmax\nscipy.special.boxcox and scipy.special.boxcox1p\nscipy.special.inv_boxcox and scipy.special.inv_boxcox1p\nsklearn.preprocessing.PowerTransformer"
  },
  {
    "objectID": "research/powertf/index.html#citation",
    "href": "research/powertf/index.html#citation",
    "title": "Power Transform Revisited: Numerically Stable, and Federated",
    "section": "Citation",
    "text": "Citation\n@misc{Xu2025powertf,\n  title={Power Transform Revisited: Numerically Stable, and Federated},\n  author={Xuefeng Xu and Graham Cormode},\n  year={2025},\n  eprint={2510.04995},\n  archivePrefix={arXiv},\n  primaryClass={cs.LG},\n  url={https://arxiv.org/abs/2510.04995},\n}"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "Power Transform Revisited: Numerically Stable, and Federated Xuefeng Xu and Graham Cormode In Submission\n\n\nFederated Computation of ROC and PR Curves Xuefeng Xu and Graham Cormode In Submission\n\n\nFedPS: Federated data Preprocessing via aggregated Statistics Code\n\n\nDeep Learning Algorithms Design and Implementation Based on Differential Privacy Xuefeng Xu, Yanqing Yao, and Lei Cheng International Conference on Machine Learning for Cyber Security, 2020"
  },
  {
    "objectID": "research/research.html",
    "href": "research/research.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "Power Transform Revisited: Numerically Stable, and Federated Xuefeng Xu and Graham Cormode In Submission\n\n\nFederated Computation of ROC and PR Curves Xuefeng Xu and Graham Cormode In Submission\n\n\nFedPS: Federated data Preprocessing via aggregated Statistics Code\n\n\nDeep Learning Algorithms Design and Implementation Based on Differential Privacy Xuefeng Xu, Yanqing Yao, and Lei Cheng International Conference on Machine Learning for Cyber Security, 2020"
  },
  {
    "objectID": "research/fedcurve/index.html",
    "href": "research/fedcurve/index.html",
    "title": "Federated Computation of ROC and PR Curves",
    "section": "",
    "text": "Paper\n  \n  \n    \n     Code"
  },
  {
    "objectID": "research/fedcurve/index.html#introduction",
    "href": "research/fedcurve/index.html#introduction",
    "title": "Federated Computation of ROC and PR Curves",
    "section": "Introduction",
    "text": "Introduction\nFederated Learning allows multiple clients to collaboratively train models without sharing raw data. However, evaluation is often limited to aggregate simple metrics such as accuracy or loss, which provide an incomplete picture of model performance.\nWe propose a method to approximate Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves in federated settings, without accessing raw client data. Our approach supports both Secure Aggregation and Differential Privacy, providing provable error guarantees and low communication cost."
  },
  {
    "objectID": "research/fedcurve/index.html#methods-overview",
    "href": "research/fedcurve/index.html#methods-overview",
    "title": "Federated Computation of ROC and PR Curves",
    "section": "Methods Overview",
    "text": "Methods Overview\nOur method has two steps: (1) each client sends local histograms of prediction scores for both classes to the server, which aggregates them to estimate quantiles; (2) the server uses these quantiles to approximate the empirical cumulative distribution functions (ECDF) and reconstructs ROC and PR curves.\n\nQuantiles Estimation via Histograms\nTo estimate quantiles, each client builds a hierarchical histogram (Figure 1) by recursively dividing the score range into equal-width bins and counting examples in each. The server aggregates these histograms and computes global quantiles based on the combined bin counts and boundaries.\n\n\n\n\n\n\nFigure 1: Hierarchical histogram structure.\n\n\n\nTo ensure client’s privacy, we consider two mechanisms:\n\nSecure Aggregation: The server sees only the total, not individual bins from clients.\nDifferential Privacy: Clients add independent noise to each bin before sending.\n\n\n\nCurve Approximation via Quantiles\nLet \\Phi^+(s) and \\Phi^-(s) be the ECDFs of prediction score distributions for positive and negative examples. We estimate Q evenly spaced quantiles (Q=6 in Figure 2), and apply monotone piecewise cubic polynomial interpolation (PCHIP) to approximate the full ECDFs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: ECDFs reconstructed from Q quantiles for both classes.\n\n\n\nFor the ROC curve, we then compute:\n\n\\begin{equation*}\nT(s)=1-\\Phi^+(s),\n\\end{equation*}\n\\tag{1}\n\n\\begin{equation*}\nF(s)=1-\\Phi^-(s),\n\\end{equation*}\n\\tag{2}\nwhere T(s) and F(s) denote the true positive rate (TPR) and false positive rate (FPR).\nFor the PR curve, recall is equivalent to TPR, and precision is computed by:\n\n\\begin{equation*}\nP(s)=\\frac{T(s)n^+}{T(s)n^+ + F(s)n^-}\n\\end{equation*}\n\\tag{3}\nHere, n^+ and n^- are the number of positive and negative examples. Figure 3 shows the resulting approximated ROC and PR curves.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: ROC and PR curves approximated from ECDFs."
  },
  {
    "objectID": "research/fedcurve/index.html#theoretical-guarantees",
    "href": "research/fedcurve/index.html#theoretical-guarantees",
    "title": "Federated Computation of ROC and PR Curves",
    "section": "Theoretical Guarantees",
    "text": "Theoretical Guarantees\nTo quantify approximation quality, we define the Area Error (AE) as:\n\nDefinition 1 AE is the integral of the absolute difference between the true and estimated curves: \n\\begin{equation*}\n\\text{AE}_\\text{ROC} = \\int_0^1 |T(f) - \\hat{T}(f)| df,\n\\end{equation*}\n\\tag{4}\n\n\\begin{equation*}\n\\text{AE}_\\text{PR} = \\int_0^1 |P(t) - \\hat{P}(t)| dt.\n\\end{equation*}\n\\tag{5}\n\nAssuming Lipschitz continuity of score distributions, we bound the AE as follows:\n\nTheorem 1 Let Q be the number of quantiles used. Then:\n\nUnder Secure Aggregation: \\text{AE}_\\text{ROC}\\le O(1/Q) and \\text{AE}_\\text{PR}\\le\\tilde{O}(1/Q).\nUnder \\varepsilon-Differential Privacy: \\text{AE}\\le\\tilde{O}(\\frac{1}{Q} + \\frac{1}{n\\varepsilon}), where n is the number of examples."
  },
  {
    "objectID": "research/fedcurve/index.html#empirical-evaluation",
    "href": "research/fedcurve/index.html#empirical-evaluation",
    "title": "Federated Computation of ROC and PR Curves",
    "section": "Empirical Evaluation",
    "text": "Empirical Evaluation\nWe evaluate the method using the Adult dataset and XGBoost classifier, We test both Secure Aggregation (SA) and Distributed Differential Privacy (DDP), varying Q from 4 to 1024 and \\varepsilon\\in\\{0.1,0.3,1\\}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Area Error of ROC and PR curves vs. number of quantiles.\n\n\n\nAs Q increases, Area Error decreases. PR curves generally show slightly higher error than ROC curves. Under DP, error plateaus due to noise, and grows as \\varepsilon decreases (stronger privacy)."
  },
  {
    "objectID": "research/fedcurve/index.html#citation",
    "href": "research/fedcurve/index.html#citation",
    "title": "Federated Computation of ROC and PR Curves",
    "section": "Citation",
    "text": "Citation\n@misc{Xu2025fedcurve,\n  title={Federated Computation of ROC and PR Curves},\n  author={Xuefeng Xu and Graham Cormode},\n  year={2025},\n  eprint={2510.04979},\n  archivePrefix={arXiv},\n  primaryClass={cs.LG},\n  url={https://arxiv.org/abs/2510.04979},\n}"
  },
  {
    "objectID": "misc/index.html",
    "href": "misc/index.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "Stop PP10043 !\nStudy Resources"
  },
  {
    "objectID": "blog/pchip.html",
    "href": "blog/pchip.html",
    "title": "Monotone Piecewise Cubic Interpolation",
    "section": "",
    "text": "Piecewise Cubic Hermite Interpolating Polynomial (PCHIP) is a cubic spline-based interpolation method designed to preserve monotonicity. See MATLAB or SciPy for the implementation details."
  },
  {
    "objectID": "blog/pchip.html#pchip-example",
    "href": "blog/pchip.html#pchip-example",
    "title": "Monotone Piecewise Cubic Interpolation",
    "section": "1 PCHIP Example",
    "text": "1 PCHIP Example\nGiven n data points (x_1,y_1),\\dots,(x_n,y_n) with x_1&lt;\\cdots&lt;x_n, where y is monotonic (either y_i\\le y_{i+1} or y_{i+1}\\ge y_i). The easiest interpolation method which preserves monotonicity is linear interpolation. However, it is not smooth. Polynomial interpolation is smooth but may introduce overshoots, violating monotonicity.\n\n\nCode\ndef plot_interp(interp_func, x, y):\n    grid_x = np.linspace(min(x), max(x), 100)\n    grid_y = interp_func(grid_x)\n\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax.plot(grid_x, grid_y)\n    ax.scatter(x, y, color='C1', zorder=2)\n\n    ax.set_xlabel(r\"$x$\")\n    ax.set_ylabel(r\"$y$\")\n\n\n\nCode\nimport numpy as np\nfrom scipy.interpolate import interp1d\nimport matplotlib.pyplot as plt\n\nx = [-3, -2, -1, 0, 1, 2, 3]\ny = [-2, -2, -2, 0, 2, 2, 2]\n\nlinear_interp = interp1d(x, y, kind=\"linear\")\nplot_interp(linear_interp, x, y)\ncubic_interp = interp1d(x, y, kind=\"cubic\")\nplot_interp(cubic_interp, x, y)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Linear\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Cubic\n\n\n\n\n\n\n\n\nFigure 1: Linear Interpolation vs. Cubic Interpolation\n\n\n\nOne of the most effective interpolation methods that preserves monotonicity while maintaining smoothness is PCHIP. Some people also refer to it as shape-preserving cubic interpolation. The following figures illustrate PCHIP interpolation, where it also appliable to non-monotonic data points.\n\nCode\nfrom scipy.interpolate import PchipInterpolator\n\nx = [-3, -2, -1, 0, 1, 2, 3]\ny = [-2, -2, -2, 0, 2, 2, 2]\n\npchip_interp = PchipInterpolator(x, y)\nplot_interp(pchip_interp, x, y)\nx = [-3, -2, -1, 0, 1, 2, 3]\ny = [-2, -1, -1.5, 0, 1, 0.5, 2]\n\npchip_interp = PchipInterpolator(x, y)\nplot_interp(pchip_interp, x, y)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) PCHIP on Monotone Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) PCHIP on Non-Monotone Data\n\n\n\n\n\n\n\n\nFigure 2: PCHIP Interpolation on Monotone and Non-Monotone Data"
  },
  {
    "objectID": "blog/pchip.html#interpolation-function",
    "href": "blog/pchip.html#interpolation-function",
    "title": "Monotone Piecewise Cubic Interpolation",
    "section": "2 Interpolation Function",
    "text": "2 Interpolation Function\nHow to ensure cubic interpolation preserves monotonicity? Let’s define some notations first. For each interval [x_i,x_{i+1}] (i=1,\\dots,n-1), define the step size h_i and slope s_i as:\n\nh_i=x_{i+1}-x_i,\\ s_i=\\frac{y_{i+1}-y_i}{h_i}\n\\tag{1}\nFor x_i&lt;x&lt;x_{i+1}, the interpolation function f(x) is a cubic polynomial:\n\nf(x)=c_0+c_1(x-x_i)+c_2(x-x_i)^2+c_3(x-x_i)^3\n\\tag{2}\nsatisfying:\n\n\\begin{align*}\nf(x_i)&=y_i, &f(x_{i+1})&=y_{i+1}\\\\\nf'(x_i)&=d_i, &f'(x_{i+1})&=d_{i+1}\n\\end{align*}\n\\tag{3}\nSolving for the coefficients:\n\n\\begin{align*}\nc_0&=y_i, &c_2&=\\frac{3s_i-2d_i-d_{i+1}}{h_i}\\\\\nc_1&=d_i, &c_3&=\\frac{-2s_i+d_i+d_{i+1}}{h_i^2}\\\\\n\\end{align*}\n\\tag{4}\nThus, computing derivatives d_1,\\dots,d_n determines c_0,c_1,c_2,c_3 for each interval."
  },
  {
    "objectID": "blog/pchip.html#derivative-formula",
    "href": "blog/pchip.html#derivative-formula",
    "title": "Monotone Piecewise Cubic Interpolation",
    "section": "3 Derivative Formula",
    "text": "3 Derivative Formula\nThe derivative d_i is computed using local information from three neighboring points \\{(x_{i-1},y_{i-1}),(x_i,y_i),(x_{i+1},y_{i+1})\\}. Replaceing with the slopes s_{i-1} and s_i, and step sizes h_{i-1} and h_i, the formula is (Fritsch and Butland 1984):\n\nd_i=G(s_{i-1},s_i,h_{i-1},h_i)=\n\\begin{cases}\n\\frac{s_{i-1}s_i}{rs_i+(1-r)s_{i-1}} & \\mathrm{if~}s_{i-1}s_i&gt;0, \\\\\n0 & \\text{otherwise} &\n\\end{cases}\n\\tag{5}\nwhere the ratio r (1/3&lt;r&lt;2/3) is given by:\n\nr=\\frac{h_{i-1}+2h_i}{3h_{i-1}+3h_i}=\\frac{1}{3}\\left(1+\\frac{h_i}{h_{i+1}+h_i}\\right)\n\\tag{6}\nIf s_{i-1} and s_i have opposite signs (indicating non-monotonicity) or one is zero, then d_i=0. Otherwise, d_i is a weighted harmonic mean:\n\n\\frac{1}{d_i}=\\frac{r}{s_{i-1}}+\\frac{1-r}{s_i}\n\\tag{7}\nFor derivatives d_1 and d_n, they are computed separately (Moler 2004, chap. 3). For d_1:\n\nd_1=\n\\begin{cases}\n0 & \\mathrm{if~}\\text{sgn}(\\hat{d}_1)\\neq \\text{sgn}(s_1), \\\\\n3s_1 & \\mathrm{if~}\\text{sgn}(s_1)\\neq \\text{sgn}(s_2) \\land|\\hat{d}_1|&gt;3|s_1|,\\\\\n\\hat{d}_1 & \\text{otherwise} &\n\\end{cases}\n\\tag{8}\nwhere:\n\n\\hat{d}_1=\\frac{(2h_1+h_2)s_1 - h_1s_2}{{h_1+h_2}}\n\\tag{9}\nA quadratic polynomial \\hat{f}(x)=\\hat{c}_0+\\hat{c}_1x+\\hat{c}_2x^2 is fit through the first three points, and \\hat{d}_1=\\hat{f}'(x_1). Additional rules are then added to preserve monotonicity. Similar procedures apply for d_n."
  },
  {
    "objectID": "blog/pchip.html#monotonicity-conditions",
    "href": "blog/pchip.html#monotonicity-conditions",
    "title": "Monotone Piecewise Cubic Interpolation",
    "section": "4 Monotonicity Conditions",
    "text": "4 Monotonicity Conditions\nTo ensure monotonicity, the derivatives d_i must satisfy certain conditions. Define:\n\n\\alpha_i=\\frac{d_i}{s_i},\\\n\\beta_i=\\frac{d_{i+1}}{s_i}\n\\tag{10}\n\nLemma 1 A sufficient condition for monotonicity is (Fritsch and Carlson 1980):\n\n\\alpha_i,\\beta_i\\ge0\n\\land\n\\left(\n\\alpha_i,\\beta_i\\le3\n\\lor\n\\phi(\\alpha_i,\\beta_i)\\ge0\n\\right)\n\\tag{11}\nwhere:\n\n\\phi(\\alpha,\\beta)=\\alpha-\\frac{(2\\alpha+\\beta-3)^2}{3(\\alpha+\\beta-2)}\n\\tag{12}\n\nTo verify Equation 5 and Equation 8 satisfy the monotonicity condition, we analyze them separately. If s_{i-1}s_i&gt;0, then \\alpha_i&gt;0; otherwise, \\alpha_i=0. Since the ratio r satisfies 1/3&lt;r&lt;2/3, \\alpha_i is upper-bounded by 3:\n\n\\alpha_i\n=\\frac{1}{rs_i/s_{i-1}+(1-r)}\n&lt;\\frac{1}{1-r}&lt;3\n\\tag{13}\nFor endpoint \\alpha_1, we only need to show the condition of \\text{sgn}(\\hat{d}_1)=\\text{sgn}(s_1)=\\text{sgn}(s_2), since other conditions already lie within the region [0,3].\n\n\\alpha_1\n=1+\\frac{1-s_2/s_1}{1+h_2/h_1}&lt;2\n\\tag{14}\nSimilarly, \\beta_i and endpoint \\beta_{n-1} all satisfy the monotonicity condition."
  },
  {
    "objectID": "blog/pchip.html#proof-of-monotonicity",
    "href": "blog/pchip.html#proof-of-monotonicity",
    "title": "Monotone Piecewise Cubic Interpolation",
    "section": "5 Proof of Monotonicity",
    "text": "5 Proof of Monotonicity\n\nProof. To preserve monotonicity, the derivatives d_i and d_{i+1} must align with the direction of the slope of the interval s_i. This is a necessary condition (Fritsch and Carlson 1980):\n\n\\text{sgn}(d_i)=\\text{sgn}(d_{i+1})=\\text{sgn}(s_i)\n\\Leftrightarrow\n\\alpha_i,\\beta_i\\ge0\n\\tag{15}\nThe derivative of f(x) is a quadratic polynomial:\n\nf'(x)=c_1+\n2c_2(x-x_i)+\n3c_3(x-x_i)^2\n\\tag{16}\nIt has a unique extremum at:\n\nx^*=x_i+\\frac{h_i}{3}\\cdot\\frac{2\\alpha_i+\\beta_i-3}{\\alpha_i+\\beta_i-2}\n\\tag{17}\nand\n\nf'(x^*)=\\phi(\\alpha_i,\\beta_i)s_i\n\\tag{18}\nThere are three conditions to check: (i) x^*&lt;x_i; (ii) x^*&gt;x_{i+1}; (iii) x_i\\le x^*\\le x_{i+1}.\nCondition (i) is equivalent to \n\\alpha_i,\\beta_i\\ge0\n\\land\n\\frac{2\\alpha_i+\\beta_i-3}{\\alpha_i+\\beta_i-2}&lt;0\n\\tag{19}\nThe analysis of Condition (ii) and (iii) are similar, leading to:\n\n\\alpha_i,\\beta_i\\ge0\n\\land\n\\frac{\\alpha_i+2\\beta_i-3}{\\alpha_i+\\beta_i-2}&lt;0\n\\tag{20}\nand\n\n\\alpha_i,\\beta_i\\ge0\n\\land\n\\frac{2\\alpha_i+\\beta_i-3}{\\alpha_i+\\beta_i-2}\\ge0\n\\land\n\\frac{\\alpha_i+2\\beta_i-3}{\\alpha_i+\\beta_i-2}\\ge0\n\\land\n\\phi(\\alpha_i,\\beta_i)\\ge0\n\\tag{21}\nFigure 3 illustrates these conditions: Condition (i) — blue, Condition (ii) — green, Condition (iii) — red.\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx_vals = np.linspace(0, 4, 1000)\ny_vals = np.linspace(0, 4, 1000)\nx, y = np.meshgrid(x_vals, y_vals)\n\ncond1 = (x &gt; 0) & (y &gt; 0) & ((2 * x + y - 3) / (x + y - 2) &lt; 0)\ncond2 = (x &gt; 0) & (y &gt; 0) & ((x + 2 * y - 3) / (x + y - 2) &lt; 0)\ncond3 = (\n    (x &gt; 0)\n    & (y &gt; 0)\n    & ((2 * x + y - 3) / (x + y - 2) &gt; 0)\n    & ((x + 2 * y - 3) / (x + y - 2) &gt; 0)\n    & (x - ((2 * x + y - 3) ** 2) / (3 * (x + y - 2)) &gt; 0)\n)\n\nfig, ax = plt.subplots(figsize=(3, 3))\n\nax.contourf(x, y, cond1, levels=1, colors=[\"white\", \"blue\"], alpha=1)\nax.contourf(x, y, cond2, levels=1, colors=[\"white\", \"green\"], alpha=0.5)\nax.contourf(x, y, cond3, levels=1, colors=[\"white\", \"red\"], alpha=0.25)\n\nax.set_xlabel(r\"$\\alpha$\")\nax.set_ylabel(r\"$\\beta$\")\nax.set_xlim([0, 4])\nax.set_ylim([0, 4])\nax.xaxis.set_ticks(list(np.arange(0, 4.5, 0.5)))\nax.yaxis.set_ticks(list(np.arange(0, 4.5, 0.5)))\n\nax.grid(True)\nax.tick_params(\n    bottom=True,\n    top=True,\n    left=True,\n    right=True,\n    labelbottom=True,\n    labeltop=True,\n    labelleft=True,\n    labelright=True,\n)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: The monotonicity region\n\n\n\n\n\n\nFinally, simplifying yields the final monotonicity condition (Lemma 1)."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nHow to Compute Variance?\n\n\n\nNumerical\n\nPython\n\n\n\nMathematically equivalent formulas for variance can have very different numerical stability.\n\n\n\n\n\nOct 3, 2025\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nROC and PR Curves\n\n\n\nData Science\n\nPython\n\n\n\nGraphical tools for evaluating classification models, highlighting trade-offs in model performance.\n\n\n\n\n\nJul 31, 2025\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nPower Transform\n\n\n\nNumerical\n\nPython\n\n\n\nParametric methods that normalize data but require careful handling to avoid numerical instability.\n\n\n\n\n\nApr 14, 2025\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\nMonotone Piecewise Cubic Interpolation\n\n\n\nStatistics\n\n\n\nShape-preserving interpolation method that preserves monotonicity and avoids overshoots.\n\n\n\n\n\nMar 13, 2025\n\n5 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/power-transform.html",
    "href": "blog/power-transform.html",
    "title": "Power Transform",
    "section": "",
    "text": "Power transforms are parametric methods that convert data into a Gaussian-like distribution. Two widely used transformations in this category are the Box-Cox (Box and Cox 1964) and Yeo-Johnson (I. Yeo and Johnson 2000) methods, both of which rely on a single parameter \\lambda."
  },
  {
    "objectID": "blog/power-transform.html#two-transformations",
    "href": "blog/power-transform.html#two-transformations",
    "title": "Power Transform",
    "section": "1 Two Transformations",
    "text": "1 Two Transformations\nThe Box-Cox transformation requires strictly positive data (x &gt; 0) and is defined as:\n\n\\psi_{\\text{BC}}(\\lambda, x) =\n\\begin{cases}\n\\frac{x^\\lambda-1}{\\lambda} & \\text{if } \\lambda\\neq0,\\\\\n\\ln x & \\text{if } \\lambda=0.\n\\end{cases}\n\\tag{1}\nThe Yeo-Johnson transformation generalizes Box-Cox to handle non-positive values and is defined as:\n\n\\psi_{\\text{YJ}}(\\lambda, x) =\n\\begin{cases}\n\\frac{(x+1)^\\lambda-1}{\\lambda} & \\text{if } \\lambda\\neq0,x\\ge0,\\\\\n\\ln(x+1) & \\text{if } \\lambda=0,x\\ge0,\\\\\n-\\frac{(-x+1)^{2-\\lambda}-1}{2-\\lambda} & \\text{if } \\lambda\\neq2,x&lt;0,\\\\\n-\\ln(-x+1) & \\text{if } \\lambda=2,x&lt;0.\\\\\n\\end{cases}\n\\tag{2}\nFigure 1 visualizes these transformations across various \\lambda values.\n\n\nCode\nimport numpy as np\nfrom scipy.special import boxcox\nfrom scipy.stats import yeojohnson\nimport matplotlib.pyplot as plt\n\ndef power_plot(x_min, x_max, power, figsize):\n    if power == \"BC\":\n        power_func = boxcox\n    else:\n        power_func = yeojohnson\n\n    eps = 0.01\n    x = np.arange(x_min, x_max, eps)\n\n    fig, ax = plt.subplots(figsize=figsize)\n    line_color = ['dodgerblue', 'limegreen', 'red', 'mediumpurple', 'orange']\n    for idx, lmb in enumerate([3, 2, 1, 0, -1]):\n        y = power_func(x, lmb)\n        ax.plot(x, y, label=fr'$\\lambda$={lmb}', color=line_color[idx])\n    \n    ax.set_xlabel(r'$x$')\n    if power == \"BC\":\n        ax.set_ylabel(r'$\\psi_{\\text{BC}}(\\lambda, x)$')\n    else:\n        ax.set_ylabel(r'$\\psi_{\\text{YJ}}(\\lambda, x)$')\n\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(-4, 4)\n\n    ax.yaxis.set_ticks(list(np.arange(-4, 5, 2)))\n    ax.get_yticklabels()[2].set(color=\"red\")\n    ax.xaxis.set_ticks(list(np.arange(x_min, x_max + 1)))\n    if power == \"BC\":\n        ax.get_xticklabels()[1].set(color=\"red\")\n    else:\n        ax.get_xticklabels()[2].set(color=\"red\")\n\n    ax.axhline(0, linestyle='--', color='k')\n    if power == \"BC\":\n        ax.axvline(1, linestyle='--', color='k')\n    else:\n        ax.axvline(0, linestyle='--', color='k')\n\n    ax.grid()\n    ax.set_aspect(0.5)\n\n    leg = ax.legend(loc='lower right')\n    leg.get_texts()[2].set(color=\"red\")\n\n    plt.show()\n\n\n\nCode\npower_plot(x_min=0, x_max=3, power=\"BC\", figsize=(2.5, 3))\npower_plot(x_min=-2, x_max=2, power=\"YJ\", figsize=(3, 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Box-Cox\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Yeo-Johnson\n\n\n\n\n\n\n\n\n\nFigure 1: Box-Cox and Yeo-Johnson transformations.\n\n\n\nThe optimal \\lambda is typically estimated by maximizing the log-likelihood. For Box-Cox and Yeo-Johnson, the respective log-likelihood functions are:\n\n\\ln\\mathcal{L}_{\\text{BC}}(\\lambda, x)=(\\lambda-1) \\sum_i^n \\ln x_i - \\frac{n}{2}\\ln\\sigma^2_{\\psi_{\\text{BC}}}\n\\tag{3}\n\n\\ln\\mathcal{L}_{\\text{YJ}}(\\lambda, x)=(\\lambda-1) \\sum_i^n \\text{sgn} (x_i) \\ln(|x_i|+1) - \\frac{n}{2}\\ln\\sigma^2_{\\psi_{\\text{YJ}}}\n\\tag{4}\nHere, \\sigma^2_\\psi represents the variance of the transformed data, \\text{Var}[\\psi(\\lambda,x)]. These log-likelihood functions are concave (Kouider and Chen 1995; Marchand et al. 2022), which guarantees a unique maximum. Brent’s method (Brent 2013) is commonly employed for optimization.\n\nfrom scipy.optimize import brent\n\ndef max_llf(x, llf):\n    def _neg_llf(lmb, x):\n        return -llf(lmb, x)\n    return brent(_neg_llf, args=(x,))"
  },
  {
    "objectID": "blog/power-transform.html#numerical-instabilities",
    "href": "blog/power-transform.html#numerical-instabilities",
    "title": "Power Transform",
    "section": "2 Numerical Instabilities",
    "text": "2 Numerical Instabilities\nBecause both transformations involve exponentiation, they are susceptible to numerical overflow. This problem has been observed by (Marchand et al. 2022) and discussed in Scikit-learn’s GitHub Issue. Below is an example illustrating the problem using a naive implementation of Equation 3:\n\ndef boxcox_llf_naive(lmb, x):\n    n = len(x)\n    logx = np.log(x)\n    logvar = np.log(np.var(boxcox(x, lmb)))\n    return (lmb - 1) * np.sum(logx) - n/2 * logvar\n\n\nx = np.array([10, 10, 10, 9.9])\nprint(max_llf(x, llf=boxcox_llf_naive))\n\n156.48528753755807\n\n\nAlthough this returns a \\lambda value, it produces overflow warnings. A useful diagnostic is to visualize the log-likelihood curve:\n\n\nCode\ndef plot_llf(x, lb, ub, llf):\n    np.set_printoptions(precision=3)\n    lmb = np.linspace(lb, ub, 20)\n    ll = np.array([llf(l, x) for l in lmb])\n    print(f\"llf={ll}\")\n\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax.plot(lmb, ll)\n    ax.set_xlabel(r\"$\\lambda$\")\n    ax.set_ylabel(r\"$\\ln\\mathcal{L}(\\lambda, x)$\")\n    plt.show()\n\n\n\nplot_llf(x, lb=150, ub=200, llf=boxcox_llf_naive)\n\nllf=[13.684 13.697 13.711   -inf   -inf   -inf   -inf   -inf   -inf   -inf\n   -inf   -inf   -inf   -inf   -inf   -inf   -inf   -inf   -inf   -inf]\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nUnfortunately, due to overflow (np.inf values), the log-likelihood curve cannot be visualized in the specified range, suggesting the returned \\lambda is not optimal."
  },
  {
    "objectID": "blog/power-transform.html#existing-solutions",
    "href": "blog/power-transform.html#existing-solutions",
    "title": "Power Transform",
    "section": "3 Existing Solutions",
    "text": "3 Existing Solutions\nThe MASS package in R (Venables and Ripley 2002) proposes a simple yet effective trick: divide the data by its mean. This rescales the data and avoids numerical instability without affecting the optimization outcome.\n\nx_dm = x / np.mean(x)\nprint(max_llf(x_dm, llf=boxcox_llf_naive))\n\n357.55141884289054\n\n\n\nplot_llf(x_dm, lb=330, ub=385, llf=boxcox_llf_naive)\n\nllf=[23.376 23.377 23.378 23.38  23.381 23.381 23.382 23.383 23.383 23.383\n 23.383 23.383 23.383 23.382 23.382 23.381 23.38  23.379 23.377 23.376]\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nTo see why this works, consider the log-variance term for \\lambda \\ne 0:\n\n\\begin{align*}\n\\ln\\text{Var}[\\psi_{\\text{BC}}(\\lambda,x)]\n&=\\ln\\text{Var}[(x^\\lambda-1)/\\lambda] \\\\\n&=\\ln\\text{Var}[x^\\lambda/\\lambda] \\\\\n&=\\ln[\\text{Var}(x^\\lambda)/\\lambda^2] \\\\\n&=\\ln\\text{Var}(x^\\lambda) - 2\\ln|\\lambda| \\\\\n\\end{align*}\n\\tag{5}\nIf x is scaled by a constant c &gt; 0:\n\n\\begin{align*}\n\\ln\\text{Var}[\\psi_{\\text{BC}}(\\lambda,x/c)]\n&=\\ln\\text{Var}[(x/c)^\\lambda] - 2\\ln|\\lambda| \\\\\n&=\\ln[\\text{Var}(x^\\lambda)/c^{2\\lambda}] - 2\\ln|\\lambda| \\\\\n&=\\ln\\text{Var}(x^\\lambda) - 2\\lambda\\ln c  - 2\\ln|\\lambda| \\\\\n&=\\ln\\text{Var}[\\psi_{\\text{BC}}(\\lambda,x)] - 2\\lambda\\ln c \\\\\n\\end{align*}\n\\tag{6}\nPlugging into the log−likelihood:\n\n\\begin{align*}\n\\ln\\mathcal{L}_{\\text{BC}}(\\lambda, x/c)\n&=(\\lambda-1) \\sum_i^n \\ln(x_i/c) - \\frac{n}{2}\\ln\\text{Var}[\\psi_{\\text{BC}}(\\lambda,x/c)] \\\\\n&=\\ln\\mathcal{L}_{\\text{BC}}(\\lambda, x) - n(\\lambda-1)\\ln c + n\\lambda\\ln c \\\\\n&=\\ln\\mathcal{L}_{\\text{BC}}(\\lambda, x) + n\\ln c \\\\\n\\end{align*}\n\\tag{7}\nThe additive constant n\\ln c does not affect the maximizer of \\lambda. However, this trick does not apply to Yeo-Johnson, where scaling alters the optimal parameter (see Equation 4)."
  },
  {
    "objectID": "blog/power-transform.html#log-space-computation",
    "href": "blog/power-transform.html#log-space-computation",
    "title": "Power Transform",
    "section": "4 Log-Space Computation",
    "text": "4 Log-Space Computation\nTo address numerical instability for both transformations, log-space computation (Haberland 2023) is effective. It uses the Log-Sum-Exp trick to compute statistics in log space:\n\nfrom scipy.special import logsumexp\n\ndef log_mean(logx):\n    # compute log of mean of x from log(x)\n    return logsumexp(logx) - np.log(len(logx))\n\ndef log_var(logx):\n    # compute log of variance of x from log(x)\n    logmean = log_mean(logx)\n    pij = np.full_like(logx, np.pi * 1j, dtype=np.complex128)\n    logxmu = logsumexp([logx, logmean + pij], axis=0)\n    return np.real(logsumexp(2 * logxmu)) - np.log(len(logx))\n\nThis allows direct computation of \\ln\\sigma^2_\\psi from \\ln x. Plugging in Equation 5, we can compute the log-likelihood in the log-space.\n\ndef boxcox_llf(lmb, x):\n    n = len(x)\n    logx = np.log(x)\n    if lmb == 0:\n        logvar = np.log(np.var(logx))\n    else:\n        logvar = log_var(lmb * logx) - 2 * np.log(abs(lmb))\n    return (lmb - 1) * np.sum(logx) - n/2 * logvar\n\nThis version avoids overflow and reliably returns the optimal \\lambda.\n\nprint(max_llf(x, llf=boxcox_llf))\n\n357.55141245531865\n\n\n\nplot_llf(x, lb=330, ub=385, llf=boxcox_llf)\n\nllf=[14.175 14.177 14.178 14.179 14.18  14.181 14.182 14.182 14.183 14.183\n 14.183 14.183 14.182 14.182 14.181 14.18  14.179 14.178 14.177 14.176]\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\nThe same principle extends to Yeo-Johnson, even for mixed-sign inputs.\n\n\nCode\ndef log_var_yeojohnson(x, lmb):\n    if np.all(x &gt;= 0):\n        if abs(lmb) &lt; np.spacing(1.0):\n            return np.log(np.var(np.log1p(x)))\n        return log_var(lmb * np.log1p(x)) - 2 * np.log(abs(lmb))\n\n    elif np.all(x &lt; 0):\n        if abs(lmb - 2) &lt; np.spacing(1.0):\n            return np.log(np.var(np.log1p(-x)))\n        return log_var((2 - lmb) * np.log1p(-x)) - 2 * np.log(abs(2 - lmb))\n\n    else:  # mixed positive and negtive data\n        logyj = np.zeros_like(x, dtype=np.complex128)\n        pos = x &gt;= 0\n\n        # when x &gt;= 0\n        if abs(lmb) &lt; np.spacing(1.0):\n            logyj[pos] = np.log(np.log1p(x[pos]) + 0j)\n        else:  # lmbda != 0\n            logm1_pos = np.full_like(x[pos], np.pi * 1j, dtype=np.complex128)\n            logyj[pos] = logsumexp(\n                [lmb * np.log1p(x[pos]), logm1_pos], axis=0\n            ) - np.log(lmb + 0j)\n\n        # when x &lt; 0\n        if abs(lmb - 2) &lt; np.spacing(1.0):\n            logyj[~pos] = np.log(-np.log1p(-x[~pos]) + 0j)\n        else:  # lmbda != 2\n            logm1_neg = np.full_like(x[~pos], np.pi * 1j, dtype=np.complex128)\n            logyj[~pos] = logsumexp(\n                [(2 - lmb) * np.log1p(-x[~pos]), logm1_neg], axis=0\n            ) - np.log(lmb - 2 + 0j)\n\n        return log_var(logyj)\n\ndef yeojohnson_llf(lmb, x):\n    n = len(x)\n    llf = (lmb - 1) * np.sum(np.sign(x) * np.log1p(np.abs(x)))\n    llf += -n / 2 * log_var_yeojohnson(x, lmb)\n    return llf"
  },
  {
    "objectID": "blog/power-transform.html#constrained-optimization",
    "href": "blog/power-transform.html#constrained-optimization",
    "title": "Power Transform",
    "section": "5 Constrained Optimization",
    "text": "5 Constrained Optimization\nEven with log-space computation, applying the transformation using a large absolute value of \\lambda can result in overflow:\n\nlmax = max_llf(x, llf=boxcox_llf)\nprint(boxcox(x, lmax))\n\n[inf inf inf inf]\n\n\nTo prevent this, we constrain \\lambda so that the transformed values stay within a safe range.\n\nLemma 1 The transformation \\psi(\\lambda,x) is monotonically increasing in both \\lambda and x (see I.-K. Yeo 1997 for Yeo-Johnson proof).\n\nThis allows us to bound \\lambda based on \\min(x) and \\max(x). For Box-Cox, we use x=1 as a threshold since \\psi_\\text{BC}(\\lambda,1)=0:\n\n\\begin{align*}\n\\max_\\lambda \\quad & \\ln\\mathcal{L}_{\\text{BC}}(\\lambda, x) \\\\\n\\textrm{s.t.} \\quad & \\lambda\\le\\psi^{-1}_{\\text{BC}}(x_{\\max},y_{\\max}) & \\text{if } x_{\\max}&gt;1 \\\\\n& \\lambda\\ge\\psi^{-1}_{\\text{BC}}(x_{\\min},-y_{\\max}) & \\text{if } x_{\\min}&lt;1 \\\\\n\\end{align*}\n\\tag{8}\nThe inverse Box−Cox function is given by:\n\n\\psi^{-1}_{\\text{BC}}(x,y)=\n-\\tfrac{1}{y}-\\tfrac{1}{\\ln x}W\\left(-\\tfrac{x^{-1/y}\\ln x}{y}\\right)\n\\tag{9}\n\nfrom scipy.special import lambertw\n\ndef boxcox_inv_lmbda(x, y):\n    num = lambertw(-(x ** (-1 / y)) * np.log(x) / y, k=-1)\n    return -1 / y - np.real(num / np.log(x))\n\n\ndef boxcox_constranined_lmax(lmax, x, ymax):\n    # x &gt; 1, boxcox(x) &gt; 0; x &lt; 1, boxcox(x) &lt; 0\n    xmin, xmax = min(x), max(x)\n    if xmin &gt;= 1:\n        x_treme = xmax\n    elif xmax &lt;= 1:\n        x_treme = xmin\n    else:  # xmin &lt; 1 &lt; xmax\n        indicator = boxcox(xmax, lmax) &gt; abs(boxcox(xmin, lmax))\n        x_treme = xmax if indicator else xmin\n    \n    if abs(boxcox(x_treme, lmax)) &gt; ymax:\n        lmax = boxcox_inv_lmbda(x_treme, ymax * np.sign(x_treme - 1))\n    return lmax\n\nThis method can be verified by testing different values for y_{\\max}:\n\ndef verify_boxcox_constranined_lmax(x):\n    np.set_printoptions(precision=3)\n    lmax = max_llf(x, llf=boxcox_llf)\n    for ymax in [1e300, 1e100, 1e30, 1e10]:\n        l = boxcox_constranined_lmax(lmax, x, ymax)\n        print(boxcox(x, l))\n\n\n# Positive overflow\nx = np.array([10, 10, 10, 9.9])\nverify_boxcox_constranined_lmax(x)\n\n[1.000e+300 1.000e+300 1.000e+300 4.783e+298]\n[1.000e+100 1.000e+100 1.000e+100 3.587e+099]\n[1.000e+30 1.000e+30 1.000e+30 7.286e+29]\n[1.00e+10 1.00e+10 1.00e+10 8.95e+09]\n\n\n\n# Negative overflow\nx = np.array([0.1, 0.1, 0.1, 0.101])\nverify_boxcox_constranined_lmax(x)\n\n[-1.00e+300 -1.00e+300 -1.00e+300 -4.93e+298]\n[-1.000e+100 -1.000e+100 -1.000e+100 -3.624e+099]\n[-1.000e+30 -1.000e+30 -1.000e+30 -7.309e+29]\n[-1.000e+10 -1.000e+10 -1.000e+10 -8.959e+09]\n\n\nThe constrained optimization approach can also be extended to Yeo-Johnson, ensures overflow-free transformations even for extreme values.\n\n\nCode\ndef yeojohnson_inv_lmbda(x, y):\n    if x &gt;= 0:\n        num = lambertw(-((x + 1) ** (-1 / y) * np.log1p(x)) / y, k=-1)\n        return -1 / y + np.real(-num / np.log1p(x))\n    else:\n        num = lambertw(((1 - x) ** (1 / y) * np.log1p(-x)) / y, k=-1)\n        return -1 / y + 2 + np.real(num / np.log1p(-x))\n\ndef yeojohnson_constranined_lmax(lmax, x, ymax):\n    # x &gt; 0, yeojohnson(x) &gt; 0; x &lt; 0, yeojohnson(x) &lt; 0\n    xmin, xmax = min(x), max(x)\n    if xmin &gt;= 0:\n        x_treme = xmax\n    elif xmax &lt;= 0:\n        x_treme = xmin\n    else:  # xmin &lt; 0 &lt; xmax\n        with np.errstate(over=\"ignore\"):\n            indicator = yeojohnson(xmax, lmax) &gt; abs(yeojohnson(xmin, lmax))\n        x_treme = xmax if indicator else xmin\n\n    with np.errstate(over=\"ignore\"):\n        if abs(yeojohnson(x_treme, lmax)) &gt; ymax:\n            lmax = yeojohnson_inv_lmbda(x_treme, ymax * np.sign(x_treme))\n    return lmax"
  },
  {
    "objectID": "teach/index.html",
    "href": "teach/index.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "Teaching Assistant\n\n@ University of Warwick\n\nCS275 Probability and Statistics: Autumn 2025\nCS429/CS909 Data Mining: Spring 2025\nCS133 Professional Skills: Autumn 2024\n\n\n\n@ Beihang University\n\n39112201 Abstract Algebra: Autumn 2019"
  }
]