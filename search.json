[
  {
    "objectID": "teach/teach.html",
    "href": "teach/teach.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "Teaching Assistant\n\n@ University of Warwick\n\nCS275 Probability and Statistics: Autumn 2025\nCS429/CS909 Data Mining: Spring 2025\nCS133 Professional Skills: Autumn 2024\n\n\n\n@ Beihang University\n\n39112201 Abstract Algebra: Autumn 2019"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "University of Warwick | Coventry, UK Ph.D. in Computer Science | September 2024 - Present\n\n\nBeihang University | Beijing, China M.E. in Cyberspace Security | September 2019 - January 2022\n\n\nJilin University | Changchun, China B.E. in Communication Engineering | September 2015 - July 2019"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "University of Warwick | Coventry, UK Ph.D. in Computer Science | September 2024 - Present\n\n\nBeihang University | Beijing, China M.E. in Cyberspace Security | September 2019 - January 2022\n\n\nJilin University | Changchun, China B.E. in Communication Engineering | September 2015 - July 2019"
  },
  {
    "objectID": "cv.html#experience",
    "href": "cv.html#experience",
    "title": "Xuefeng Xu",
    "section": "Experience",
    "text": "Experience\n\n\nPrimiHub | Beijing, China Privacy Engineer | December 2022 - August 2024\n\n\nOPPO | Shenzhen, China Machine Learning Intern | May 2022 - August 2022"
  },
  {
    "objectID": "cv.html#research",
    "href": "cv.html#research",
    "title": "Xuefeng Xu",
    "section": "Research",
    "text": "Research\n\n\nPower Transform Revisited: Numerically Stable, and Federated Xuefeng Xu and Graham Cormode In Submission\n\n\nFederated Computation of ROC and PR Curves Xuefeng Xu and Graham Cormode In Submission\n\n\nFedPS: Federated data Preprocessing via aggregated Statistics Code\n\n\nDeep Learning Algorithms Design and Implementation Based on Differential Privacy Xuefeng Xu, Yanqing Yao, and Lei Cheng International Conference on Machine Learning for Cyber Security, 2020"
  },
  {
    "objectID": "cv.html#teaching",
    "href": "cv.html#teaching",
    "title": "Xuefeng Xu",
    "section": "Teaching",
    "text": "Teaching\n\nTeaching Assistant\n\n@ University of Warwick\n\nCS275 Probability and Statistics: Autumn 2025\nCS429/CS909 Data Mining: Spring 2025\nCS133 Professional Skills: Autumn 2024\n\n\n\n@ Beihang University\n\n39112201 Abstract Algebra: Autumn 2019"
  },
  {
    "objectID": "cv.html#service",
    "href": "cv.html#service",
    "title": "Xuefeng Xu",
    "section": "Service",
    "text": "Service\n\nConference Reviewer\n\nInternational Conference on Learning Representations (ICLR): 2025"
  },
  {
    "objectID": "blog/roc-pr-curve.html",
    "href": "blog/roc-pr-curve.html",
    "title": "ROC and PR Curves",
    "section": "",
    "text": "Receiver Operating Characteristic (ROC) and Precision–Recall (PR) curves are essential for evaluating classification models. They visualize trade-offs between True Positive Rate (TPR) and False Positive Rate (FPR) (ROC) or Precision and Recall (PR), offering deeper insights than single scalar metrics."
  },
  {
    "objectID": "blog/roc-pr-curve.html#roc-curve",
    "href": "blog/roc-pr-curve.html#roc-curve",
    "title": "ROC and PR Curves",
    "section": "1 ROC Curve",
    "text": "1 ROC Curve\nFor binary classification, with Positive (P) and Negative (N) classes, predictions are based on model scores thresholded to produce labels. Performance is summarized using a confusion matrix (Table 1), counting True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN).\n\n\n\nTable 1: The Confusion Matrix\n\n\n\n\n\nTotal=P+N\nPredicted Pos (PP)\nPredicted Neg (PN)\n\n\n\n\n\nActual Pos (P)\nTP\nFN\nP=TP+FN\n\n\nActual Neg (N)\nFP\nTN\nN=FP+TN\n\n\n\nPP=TP+FP\nPN=FN+TN\n\n\n\n\n\n\n\nThe ROC curve plots TPR vs. FPR, where:\n\n\\text{TPR}=\\text{TP}/\\text{P},\\quad \\text{FPR}=\\text{FP}/\\text{N}\n\\tag{1}\nA simple algorithm constructs the ROC curve: 1) Sort predictions in descending score order. 2) Sweep thresholds across scores, updating TP and FP counts (Fawcett 2006).\n\nimport numpy as np\n\ndef count_fp_tp(y_true, y_score):\n    fp, tp = 0, 0\n    fps, tps, thresholds = [], [], []\n    score = np.inf\n\n    for i in np.flip(np.argsort(y_score)):\n1        if y_score[i] != score:\n            fps.append(fp)\n            tps.append(tp)\n            thresholds.append(score)\n            score = y_score[i]\n\n        if y_true[i] == 1:\n            tp += 1\n        else:\n            fp += 1\n\n    fps.append(fp)\n    tps.append(tp)\n    thresholds.append(score)\n    return np.asarray(fps), np.asarray(tps), np.asarray(thresholds)\n\n\n1\n\nTo handle score repetitions, record (FP, TP) counts only when the score changes.\n\n\n\n\n\ndef roc_curve(y_true, y_score):\n    fps, tps, thresholds = count_fp_tp(y_true, y_score)\n    fpr = fps / sum(y_true == 0)\n    tpr = tps / sum(y_true == 1)\n    return fpr, tpr, thresholds\n\nThe ROC curve is always monotonically non-decreasing, starting at (0, 0) for the highest threshold and ending at (1, 1) for the lowest threshold. Points are connected using linear interpolation. A random classifier lies along y=x, while curves closer to the top-left indicate better performance.\n\n\nCode\nimport matplotlib.pyplot as plt\n\ndef plot_curve(x, y, curve: str, pos_frac=None, drawstyle=\"default\", ax=None):\n    if ax is None:\n        _, ax = plt.subplots(figsize=(3, 3))\n\n    name = f\"{curve} curve\"\n    ax.plot(x, y, \"x-\", c=\"C0\", label=name, drawstyle=drawstyle)\n    ax.set_title(name)\n\n    ax.set_aspect(\"equal\")\n    ax.set_xlim(-0.01, 1.01)\n    ax.set_ylim(-0.01, 1.01)\n    \n    chance = {\n        \"label\": \"Chance level\",\n        \"color\": \"k\",\n        \"linestyle\": \"--\",\n    }\n    if curve == \"ROC\":\n        ax.plot([0, 1], [0, 1], **chance)\n        ax.set_xlabel(\"FPR\")\n        ax.set_ylabel(\"TPR\")\n        ax.legend(loc=\"lower right\")\n    elif curve == \"PR\":  # curve == \"PR\":\n        if pos_frac is not None:\n            ax.plot([0, 1], [pos_frac, pos_frac], **chance)\n        ax.set_xlabel(\"Recall\")\n        ax.set_ylabel(\"Precision\")\n        ax.legend(loc=\"lower left\")\n    elif curve == \"PRG\":\n        ax.set_xlabel(\"Recall Gain\")\n        ax.set_ylabel(\"Precision Gain\")\n        ax.legend(loc=\"lower left\")\n    else:\n        raise ValueError(f\"Unknown curve type: {curve}\")\n\n\n\ny_true = np.array([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0])\ny_score = np.array([\n    .9, .8, .7, .6, .55, .54, .53, .52, .51, .505,\n    .4, .39, .38, .37, .36, .35, .34, .33, .3, .1\n])\n\nfpr, tpr, _ = roc_curve(y_true, y_score)\nplot_curve(fpr, tpr, \"ROC\")"
  },
  {
    "objectID": "blog/roc-pr-curve.html#roc-convex-hull",
    "href": "blog/roc-pr-curve.html#roc-convex-hull",
    "title": "ROC and PR Curves",
    "section": "2 ROC Convex Hull",
    "text": "2 ROC Convex Hull\nThe ROC Convex Hull (ROCCH) highlights potentially optimal performance of a classifier (or a set of classifiers) by connecting upper boundary points (Provost and Fawcett 2001). It can be computed via algorithms like the Monotone chain algorithm.\n\ndef convex_hull(points):\n    points = sorted(set(points))\n\n    def cross(o, a, b):\n        return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0])\n\n    # Build upper hull\n    upper = []\n    for p in reversed(points):\n        while len(upper) &gt;= 2 and cross(upper[-2], upper[-1], p) &lt;= 0:\n            upper.pop()\n        upper.append(p)\n    \n    return upper\n\n\n\nrocch = convex_hull(zip(fpr, tpr))\n\nrocch = np.array(rocch)\nfpr_ch, tpr_ch = rocch[:, 0], rocch[:, 1]\n\nfig, ax = plt.subplots(figsize=(3, 3))\nax.plot(fpr_ch, tpr_ch, 'C1-.', label=\"ROCCH\")\nplot_curve(fpr, tpr, \"ROC\", ax=ax)"
  },
  {
    "objectID": "blog/roc-pr-curve.html#pr-curve",
    "href": "blog/roc-pr-curve.html#pr-curve",
    "title": "ROC and PR Curves",
    "section": "3 PR Curve",
    "text": "3 PR Curve\nPR curves plot Precision vs. Recall. Recall equals TPR, while Precision is:\n\n\\text{Prec}=\\frac{\\text{TP}}{\\text{TP}+\\text{FP}}\n\\tag{2}\nPR curves start at (0, 1) and end at (1, \\frac{\\text{P}}{\\text{P}+\\text{N}}), where the endpoint reflects the proportion of positive samples. Because Precision depends on both TP and FP, the PR curve is non-monotonic and can decrease as Recall increases. Moreover, it is highly influenced by class imbalance (Williams 2021). For a random classifier, the PR curve is a horizontal line at y = \\frac{\\text{P}}{\\text{P}+\\text{N}}, whereas curves closer to the top-right indicate stronger performance.\n\ndef pr_curve(y_true, y_score):\n    fps, tps, thresholds = count_fp_tp(y_true, y_score)\n\n    pps = fps + tps\n    precision = np.ones_like(tps, dtype=np.float64)\n    np.divide(tps, pps, out=precision, where=(pps != 0))\n\n    recall = tps / sum(y_true == 1)\n    return np.flip(precision), np.flip(recall), np.flip(thresholds)\n\nAlthough ROC and PR curves are mathematically related, linear interpolation is incorrect for PR curves, as it yields overly optimistic estimates of performance (Davis and Goadrich 2006). Precision does not necessarily vary linearly with Recall, so naive interpolation misrepresents true model behavior.\nTo illustrate this, we convert the ROC Convex Hull (ROCCH) into PR space, treating it as the potential optimal PR curve. Using Equation 3, we express Precision in terms of TPR and FPR:\n\n\\text{Prec}=\\frac{\\text{P}\\cdot\\text{TPR}}{\\text{P}\\cdot\\text{TPR}+\\text{N}\\cdot\\text{FPR}}\n=\\frac{1}{1+\\frac{\\text{N}}{\\text{P}}\\cdot\\frac{\\text{FPR}}{\\text{TPR}}}\n\\tag{3}\n\nfrom scipy.interpolate import interp1d\n\ndef pr_from_roc(fpr, tpr, neg_to_pos):\n    fpr_tpr_func = interp1d(tpr, fpr, bounds_error=False, fill_value=(0, 1))\n\n    def prec_func(tpr):\n        fpr = fpr_tpr_func(tpr)\n        fpr_to_tpr = np.zeros_like(tpr, dtype=np.float64)\n        np.divide(fpr, tpr, out=fpr_to_tpr, where=(tpr != 0))\n        prec = 1 / (1 + neg_to_pos * fpr_to_tpr)\n        return prec\n\n    return prec_func\n\n\nneg_to_pos = sum(y_true == 0) / sum(y_true == 1)\nprec_ch_func = pr_from_roc(fpr_ch, tpr_ch, neg_to_pos)\n\nfig, ax = plt.subplots(figsize=(3, 3))\nax.plot(tpr_ch, prec_ch_func(tpr_ch), 'C2:', label=\"Linear interp ROCCH\")\n\nx = np.linspace(0, 1, 100)\nax.plot(x, prec_ch_func(x), 'C1-.', label=\"ROCCH\")\n\nprecision, recall, _ = pr_curve(y_true, y_score)\nax.scatter(recall, precision, c=\"C0\", marker=\"x\", label=\"PR points\")\n\nax.set_aspect(\"equal\")\nax.set_xlim(-0.01, 1.01)\nax.set_ylim(-0.01, 1.01)\nax.set_xlabel(\"Recall\")\nax.set_ylabel(\"Precision\")\nax.legend(loc=\"lower left\")\n\n\n\n\n\n\n\n\nThe converted ROCCH dominates the PR space but is clearly non-linear, lying beneath its own linear interpolation (dotted line). This confirms that PR curves cannot be linearly interpolated. Scikit-learn’s PrecisionRecallDisplay instead uses step-wise interpolation, consistent with average_precision_score. This ensures that the area under the PR curve equals the average precision.\n\npos_frac = sum(y_true == 1) / len(y_true)\nplot_curve(recall, precision, \"PR\", pos_frac, drawstyle=\"steps-post\")"
  },
  {
    "objectID": "blog/roc-pr-curve.html#prg-curve",
    "href": "blog/roc-pr-curve.html#prg-curve",
    "title": "ROC and PR Curves",
    "section": "4 PRG Curve",
    "text": "4 PRG Curve\nROC curves benefit from linear interpolation and universal baselines, whereas PR curves lack these properties. To address this, Flach and Kull (2015) proposed the Precision–Recall-Gain (PRG) curve, which transforms Precision and Recall into Gains and plots these Precision Gain and Recall Gain within the unit square:\n\n\\text{PrecG}=\\frac{\\text{Prec}-\\pi}{(1-\\pi)\\text{Prec}},\\quad\n\\text{RecG}=\\frac{\\text{Rec}-\\pi}{(1-\\pi)\\text{Rec}}\n\\tag{4}\nwhere \\pi=\\frac{\\text{P}}{\\text{P}+\\text{N}} is the positive class fraction. Notably, Gains can be negative, and such points are typically omitted from the PRG curve.\n\ndef prg_curve(y_true, y_score):\n    prec, rec, thresholds = pr_curve(y_true, y_score)\n    pos_frac = sum(y_true == 0) / len(y_true)\n\n    # Remove negative gains\n    mask = (rec &gt;= pos_frac) & (prec &gt;= pos_frac)\n    prec = prec[mask]\n    rec = rec[mask]\n\n    prec_gain = (prec - pos_frac) / (1 - pos_frac) / prec\n    rec_gain = (rec - pos_frac) / (1 - pos_frac) / rec\n    return prec_gain, rec_gain, thresholds\n\n\nprec_gain, rec_gain, _ = prg_curve(y_true, y_score)\nplot_curve(rec_gain, prec_gain, \"PRG\")"
  },
  {
    "objectID": "blog/pchip.html",
    "href": "blog/pchip.html",
    "title": "Monotone Piecewise Cubic Interpolation",
    "section": "",
    "text": "Piecewise Cubic Hermite Interpolating Polynomial (PCHIP) is a cubic spline-based interpolation method designed to preserve monotonicity. See MATLAB or SciPy for the implementation details."
  },
  {
    "objectID": "blog/pchip.html#interpolation-function",
    "href": "blog/pchip.html#interpolation-function",
    "title": "Monotone Piecewise Cubic Interpolation",
    "section": "1 Interpolation Function",
    "text": "1 Interpolation Function\nGiven n data points (x_1,y_1),\\dots,(x_n,y_n) with x_1&lt;\\cdots&lt;x_n, where y is monotonic (either y_i\\le y_{i+1} or y_{i+1}\\ge y_i), define:\n\nh_i=x_{i+1}-x_i,\\ s_i=\\frac{y_{i+1}-y_i}{h_i}\n\\tag{1}\nFor x_i&lt;x&lt;x_{i+1}, the interpolation function f(x) is a cubic polynomial:\n\nf(x)=c_0+c_1(x-x_i)+c_2(x-x_i)^2+c_3(x-x_i)^3\n\\tag{2}\nsatisfying:\n\n\\begin{align*}\nf(x_i)&=y_i, &f(x_{i+1})&=y_{i+1}\\\\\nf'(x_i)&=d_i, &f'(x_{i+1})&=d_{i+1}\n\\end{align*}\n\\tag{3}\nSolving for the coefficients:\n\n\\begin{align*}\nc_0&=y_i, &c_2&=\\frac{3s_i-2d_i-d_{i+1}}{h_i}\\\\\nc_1&=d_i, &c_3&=\\frac{-2s_i+d_i+d_{i+1}}{h_i^2}\\\\\n\\end{align*}\n\\tag{4}\nThus, computing derivatives d_1,\\dots,d_n determines c_0,c_1,c_2,c_3 for each interval."
  },
  {
    "objectID": "blog/pchip.html#derivative-computation",
    "href": "blog/pchip.html#derivative-computation",
    "title": "Monotone Piecewise Cubic Interpolation",
    "section": "2 Derivative Computation",
    "text": "2 Derivative Computation\nThe derivative at x_i is computed using local information from three neighboring points (Fritsch and Butland 1984):\n\nd_i=G(s_{i-1},s_i,h_{i-1},h_i)=\n\\begin{cases}\n\\frac{s_{i-1}s_i}{rs_i+(1-r)s_{i-1}} & \\mathrm{if~}s_{i-1}s_i&gt;0, \\\\\n0 & \\text{otherwise} &\n\\end{cases}\n\\tag{5}\nwhere the ratio r (1/3&lt;r&lt;2/3) is given by:\n\nr=\\frac{h_{i-1}+2h_i}{3h_{i-1}+3h_i}=\\frac{1}{3}\\Big(1+\\frac{h_i}{h_{i+1}+h_i}\\Big)\n\\tag{6}\nIf s_{i-1} and s_i have opposite signs (indicating non-monotonicity) or one is zero, then d_i=0. Otherwise, d_i is a weighted harmonic mean:\n\n\\frac{1}{d_i}=\\frac{r}{s_{i-1}}+\\frac{1-r}{s_i}\n\\tag{7}\nFor endpoints x_1 and x_n, derivatives are computed separately (Moler 2004, chap. 3):\n\nd_1=\n\\begin{cases}\n0 & \\mathrm{if~}\\text{sgn}(\\hat{d}_1)\\neq \\text{sgn}(s_1), \\\\\n3s_1 & \\mathrm{if~}\\text{sgn}(s_1)\\neq \\text{sgn}(s_2) \\land|\\hat{d}_1|&gt;3|s_1|,\\\\\n\\hat{d}_1 & \\text{otherwise} &\n\\end{cases}\n\\tag{8}\nwhere:\n\n\\hat{d}_1=\\frac{(2h_1+h_2)s_1 - h_1s_2}{{h_1+h_2}}\n\\tag{9}\nA quadratic polynomial \\hat{f}(x)=\\hat{c}_0+\\hat{c}_1x+\\hat{c}_2x^2 is fit through the first three points, and its derivative at x_1 is computed to obtain \\hat{d}_1. Additional rules are then applied to preserve monotonicity. Similar rules apply for d_n."
  },
  {
    "objectID": "blog/pchip.html#monotonicity-conditions",
    "href": "blog/pchip.html#monotonicity-conditions",
    "title": "Monotone Piecewise Cubic Interpolation",
    "section": "3 Monotonicity Conditions",
    "text": "3 Monotonicity Conditions\nTo ensure monotonicity, define:\n\n\\alpha_i=\\frac{d_i}{s_i},\\\n\\beta_i=\\frac{d_{i+1}}{s_i}\n\\tag{10}\n\nLemma 1 A sufficient condition for monotonicity is (Fritsch and Carlson 1980):\n\n\\alpha_i,\\beta_i\\ge0\n\\land\n\\Big(\n\\alpha_i,\\beta_i\\le3\n\\lor\n\\phi(\\alpha_i,\\beta_i)\\ge0\n\\Big)\n\\tag{11}\nwhere:\n\n\\phi(\\alpha,\\beta)=\\alpha-\\frac{(2\\alpha+\\beta-3)^2}{3(\\alpha+\\beta-2)}\n\\tag{12}\n\nIf s_{i-1}s_i&gt;0, then \\alpha_i&gt;0; otherwise, \\alpha_i=0. Since the ratio r satisfies 1/3&lt;r&lt;2/3, \\alpha_i is upper-bounded by 3:\n\n\\alpha_i\n=\\frac{1}{rs_i/s_{i-1}+(1-r)}\n&lt;\\frac{1}{1-r}&lt;3\n\\tag{13}\nFor endpoint \\alpha_1, we only need to show the condition of \\text{sgn}(\\hat{d}_1)=\\text{sgn}(s_1)=\\text{sgn}(s_2), since other conditions already lie within the region [0,3].\n\n\\alpha_1\n=1+\\frac{1-s_2/s_1}{1+h_2/h_1}&lt;2\n\\tag{14}\nSimilarly, \\beta_i and endpoint \\beta_{n-1} all satisfy the monotonicity condition."
  },
  {
    "objectID": "blog/pchip.html#proof-of-monotonicity",
    "href": "blog/pchip.html#proof-of-monotonicity",
    "title": "Monotone Piecewise Cubic Interpolation",
    "section": "4 Proof of Monotonicity",
    "text": "4 Proof of Monotonicity\n\nProof. To preserve monotonicity, the derivatives d_i and d_{i+1} must align with the direction of the slope of the interval s_i. This is a necessary condition (Fritsch and Carlson 1980):\n\n\\text{sgn}(d_i)=\\text{sgn}(d_{i+1})=\\text{sgn}(s_i)\n\\Leftrightarrow\n\\alpha_i,\\beta_i\\ge0\n\\tag{15}\nThe derivative of f(x) is a quadratic polynomial:\n\nf'(x)=c_1+\n2c_2(x-x_i)+\n3c_3(x-x_i)^2\n\\tag{16}\nIt has a unique extremum at:\n\nx^*=x_i+\\frac{h_i}{3}\\cdot\\frac{2\\alpha_i+\\beta_i-3}{\\alpha_i+\\beta_i-2}\n\\tag{17}\nand\n\nf'(x^*)=\\phi(\\alpha_i,\\beta_i)s_i\n\\tag{18}\nThere are three conditions to check: 1) x^*&lt;x_i; 2) x^*&gt;x_{i+1}; 3) x_i\\le x^*\\le x_{i+1}.\nCondition 1) is equivalent to \n\\alpha_i,\\beta_i\\ge0\n\\land\n\\frac{2\\alpha_i+\\beta_i-3}{\\alpha_i+\\beta_i-2}&lt;0\n\\tag{19}\nThe analysis of Condition 2) and 3) are similar, leading to:\n\n\\alpha_i,\\beta_i\\ge0\n\\land\n\\frac{\\alpha_i+2\\beta_i-3}{\\alpha_i+\\beta_i-2}&lt;0\n\\tag{20}\nand\n\n\\alpha_i,\\beta_i\\ge0\n\\land\n\\frac{2\\alpha_i+\\beta_i-3}{\\alpha_i+\\beta_i-2}\\ge0\n\\land\n\\frac{\\alpha_i+2\\beta_i-3}{\\alpha_i+\\beta_i-2}\\ge0\n\\land\n\\phi(\\alpha_i,\\beta_i)\\ge0\n\\tag{21}\nFigure 1 illustrates these conditions separately: Condition 1) — blue, Condition 2) — green, Condition 3) — red.\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx_vals = np.linspace(0, 4, 1000)\ny_vals = np.linspace(0, 4, 1000)\nx, y = np.meshgrid(x_vals, y_vals)\n\ncond1 = (x &gt; 0) & (y &gt; 0) & ((2 * x + y - 3) / (x + y - 2) &lt; 0)\ncond2 = (x &gt; 0) & (y &gt; 0) & ((x + 2 * y - 3) / (x + y - 2) &lt; 0)\ncond3 = (\n    (x &gt; 0)\n    & (y &gt; 0)\n    & ((2 * x + y - 3) / (x + y - 2) &gt; 0)\n    & ((x + 2 * y - 3) / (x + y - 2) &gt; 0)\n    & (x - ((2 * x + y - 3) ** 2) / (3 * (x + y - 2)) &gt; 0)\n)\n\nfig, ax = plt.subplots(figsize=(3, 3))\n\nax.contourf(x, y, cond1, levels=1, colors=[\"white\", \"blue\"], alpha=1)\nax.contourf(x, y, cond2, levels=1, colors=[\"white\", \"green\"], alpha=0.5)\nax.contourf(x, y, cond3, levels=1, colors=[\"white\", \"red\"], alpha=0.25)\n\nax.set_xlabel(r\"$\\alpha$\")\nax.set_ylabel(r\"$\\beta$\")\nax.set_xlim([0, 4])\nax.set_ylim([0, 4])\nax.xaxis.set_ticks(list(np.arange(0, 4.5, 0.5)))\nax.yaxis.set_ticks(list(np.arange(0, 4.5, 0.5)))\n\nax.grid(True)\nax.tick_params(\n    bottom=True,\n    top=True,\n    left=True,\n    right=True,\n    labelbottom=True,\n    labeltop=True,\n    labelleft=True,\n    labelright=True,\n)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: The monotonicity region\n\n\n\n\n\n\nFinally, simplifying yields the final monotonicity condition (Lemma 1)."
  },
  {
    "objectID": "blog/pchip.html#cubic-hermite-interpolation",
    "href": "blog/pchip.html#cubic-hermite-interpolation",
    "title": "Monotone Piecewise Cubic Interpolation",
    "section": "5 Cubic Hermite Interpolation",
    "text": "5 Cubic Hermite Interpolation\nCubic Hermite Interpolation constructs f(x) using both function values and derivatives:\n\nf(x)=y_iH_1(x)+y_{i+1}H_2(x)+d_iH_3(x)+d_{i+1}H_4(x)\n\\tag{22}\nwhere basis functions are:\n\n\\begin{align*}\nH_{1}(x)&=\\phi\\Big(\\frac{x_{i+1}-x}{h_{i}}\\Big), &H_{2}(x)&=\\phi\\Big(\\frac{x-x_{i}}{h_{i}}\\Big)\\\\\nH_{3}(x)&=-h_{i}\\psi\\Big(\\frac{x_{i+1}-x}{h_{i}}\\Big), &H_{4}(x)&=h_{i}\\psi\\Big(\\frac{x-x_{i}}{h_{i}}\\Big)\n\\end{align*}\n\\tag{23}\nwith:\n\n\\phi(t)=3t^{2}-2t^{3},\\  \\psi(t)=t^{3}-t^{2}\n\\tag{24}\nThis formulation aligns with the cubic polynomial definition above."
  },
  {
    "objectID": "misc/index.html",
    "href": "misc/index.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "Stop PP10043 !\nStudy Resources"
  },
  {
    "objectID": "research/research.html",
    "href": "research/research.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "Power Transform Revisited: Numerically Stable, and Federated Xuefeng Xu and Graham Cormode In Submission\n\n\nFederated Computation of ROC and PR Curves Xuefeng Xu and Graham Cormode In Submission\n\n\nFedPS: Federated data Preprocessing via aggregated Statistics Code\n\n\nDeep Learning Algorithms Design and Implementation Based on Differential Privacy Xuefeng Xu, Yanqing Yao, and Lei Cheng International Conference on Machine Learning for Cyber Security, 2020"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "Power Transform Revisited: Numerically Stable, and Federated Xuefeng Xu and Graham Cormode In Submission\n\n\nFederated Computation of ROC and PR Curves Xuefeng Xu and Graham Cormode In Submission\n\n\nFedPS: Federated data Preprocessing via aggregated Statistics Code\n\n\nDeep Learning Algorithms Design and Implementation Based on Differential Privacy Xuefeng Xu, Yanqing Yao, and Lei Cheng International Conference on Machine Learning for Cyber Security, 2020"
  },
  {
    "objectID": "misc/resource.html",
    "href": "misc/resource.html",
    "title": "Study Resources",
    "section": "",
    "text": "Books\n\nIntroduction to Linear Algebra\nAlgorithms\nNumerical Recipes: The Art of Scientific Computing\nPattern Recognition and Machine Learning\nThe Elements of Statistical Learning\nProbabilistic Machine Learning\nFoundations of Machine Learning\nThe Algorithmic Foundations of Differential Privacy\nThe Complexity of Differential Privacy\nIntroduction to Modern Cryptography\nEffective Python\n\n\n\nCourses\n\nMIT 18.06 Linear Algebra\nCoursera Algorithms Part 1 and Part 2\nStanford CS229 Machine Learning\nCoursera Deep Learning Specialization\nCoursera Cryptography\nMIT The Missing Semester of Your CS Education"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Xuefeng Xu 许雪峰",
    "section": "",
    "text": "Email: xuefeng.xu@warwick.ac.uk\n \n  \n   \n  \n    \n     GitHub\n  \n  \n      Google Scholar\n  \n\n\n\n\nBiography\nI am a Ph.D. student in Computer Science at the University of Warwick, supervised by Graham Cormode and collaborating with Hakan Ferhatosmanoglu. My research interests are privacy-preserving algorithms, particularly in the context of privacy-preserving machine learning. I received my M.E. from Beihang University and B.E. from Jilin University.\n\n\nNews\n\n09/2025: Volunteer at VLDB Conference 2025\n08/2025: Poster presentation at CISPA - ELLIS Summer School 2025\n03/2025: Online talk at Flower AI Summit 2025\n02/2025: Lightning talk at Privacy in Machine Learning Meetup"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nROC and PR Curves\n\n\n\nData Science\n\nPython\n\n\n\nROC and PR curves are graphical tools for evaluating classification models, highlighting trade-offs in model performance.\n\n\n\n\n\nJul 31, 2025\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nPower Transform\n\n\n\nNumerical\n\nPython\n\n\n\nBox-Cox and Yeo-Johnson are power transformations that normalize data but require careful handling to avoid numerical instability.\n\n\n\n\n\nApr 14, 2025\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\nMonotone Piecewise Cubic Interpolation\n\n\n\nStatistics\n\n\n\nPCHIP is a cubic interpolation method that preserves monotonicity by computing shape-preserving derivatives to avoid overshoots.\n\n\n\n\n\nMar 13, 2025\n\n3 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/power-transform.html",
    "href": "blog/power-transform.html",
    "title": "Power Transform",
    "section": "",
    "text": "Power transforms are parametric methods that convert data into a Gaussian-like distribution. Two widely used transformations in this category are the Box-Cox (Box and Cox 1964) and Yeo-Johnson (I. Yeo and Johnson 2000) methods, both of which rely on a single parameter \\lambda."
  },
  {
    "objectID": "blog/power-transform.html#two-transformations",
    "href": "blog/power-transform.html#two-transformations",
    "title": "Power Transform",
    "section": "1 Two Transformations",
    "text": "1 Two Transformations\nThe Box-Cox transformation requires strictly positive data (x &gt; 0) and is defined as:\n\n\\psi_{\\text{BC}}(\\lambda, x) =\n\\begin{cases}\n\\frac{x^\\lambda-1}{\\lambda} & \\text{if } \\lambda\\neq0,\\\\\n\\ln x & \\text{if } \\lambda=0.\n\\end{cases}\n\\tag{1}\nThe Yeo-Johnson transformation generalizes Box-Cox to handle non-positive values and is defined as:\n\n\\psi_{\\text{YJ}}(\\lambda, x) =\n\\begin{cases}\n\\frac{(x+1)^\\lambda-1}{\\lambda} & \\text{if } \\lambda\\neq0,x\\ge0,\\\\\n\\ln(x+1) & \\text{if } \\lambda=0,x\\ge0,\\\\\n-\\frac{(-x+1)^{2-\\lambda}-1}{2-\\lambda} & \\text{if } \\lambda\\neq2,x&lt;0,\\\\\n-\\ln(-x+1) & \\text{if } \\lambda=2,x&lt;0.\\\\\n\\end{cases}\n\\tag{2}\nFigure 1 visualizes these transformations across various \\lambda values.\n\n\nCode\nimport numpy as np\nfrom scipy.special import boxcox\nfrom scipy.stats import yeojohnson\nimport matplotlib.pyplot as plt\n\ndef power_plot(x_min, x_max, power, figsize):\n    if power == \"BC\":\n        power_func = boxcox\n    else:\n        power_func = yeojohnson\n\n    eps = 0.01\n    x = np.arange(x_min, x_max, eps)\n\n    fig, ax = plt.subplots(figsize=figsize)\n    line_color = ['dodgerblue', 'limegreen', 'red', 'mediumpurple', 'orange']\n    for idx, lmb in enumerate([3, 2, 1, 0, -1]):\n        y = power_func(x, lmb)\n        ax.plot(x, y, label=fr'$\\lambda$={lmb}', color=line_color[idx])\n    \n    ax.set_xlabel(r'$x$')\n    if power == \"BC\":\n        ax.set_ylabel(r'$\\psi_{\\text{BC}}(\\lambda, x)$')\n    else:\n        ax.set_ylabel(r'$\\psi_{\\text{YJ}}(\\lambda, x)$')\n\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(-4, 4)\n\n    ax.yaxis.set_ticks(list(np.arange(-4, 5, 2)))\n    ax.get_yticklabels()[2].set(color=\"red\")\n    ax.xaxis.set_ticks(list(np.arange(x_min, x_max + 1)))\n    if power == \"BC\":\n        ax.get_xticklabels()[1].set(color=\"red\")\n    else:\n        ax.get_xticklabels()[2].set(color=\"red\")\n\n    ax.axhline(0, linestyle='--', color='k')\n    if power == \"BC\":\n        ax.axvline(1, linestyle='--', color='k')\n    else:\n        ax.axvline(0, linestyle='--', color='k')\n\n    ax.grid()\n    ax.set_aspect(0.5)\n\n    leg = ax.legend(loc='lower right')\n    leg.get_texts()[2].set(color=\"red\")\n\n    plt.show()\n\n\n\nCode\npower_plot(x_min=0, x_max=3, power=\"BC\", figsize=(2.5, 3))\npower_plot(x_min=-2, x_max=2, power=\"YJ\", figsize=(3, 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Box-Cox\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Yeo-Johnson\n\n\n\n\n\n\n\n\n\nFigure 1: Box-Cox and Yeo-Johnson transformations.\n\n\n\nThe optimal \\lambda is typically estimated by maximizing the log-likelihood. For Box-Cox and Yeo-Johnson, the respective log-likelihood functions are:\n\n\\ln\\mathcal{L}_{\\text{BC}}(\\lambda, x)=(\\lambda-1) \\sum_i^n \\ln x_i - \\frac{n}{2}\\ln\\sigma^2_{\\psi_{\\text{BC}}}\n\\tag{3}\n\n\\ln\\mathcal{L}_{\\text{YJ}}(\\lambda, x)=(\\lambda-1) \\sum_i^n \\text{sgn} (x_i) \\ln(|x_i|+1) - \\frac{n}{2}\\ln\\sigma^2_{\\psi_{\\text{YJ}}}\n\\tag{4}\nHere, \\sigma^2_\\psi represents the variance of the transformed data, \\text{Var}[\\psi(\\lambda,x)]. These log-likelihood functions are concave (Kouider and Chen 1995; Marchand et al. 2022), which guarantees a unique maximum. Brent’s method (Brent 2013) is commonly employed for optimization.\n\nfrom scipy.optimize import brent\n\ndef max_llf(x, llf):\n    def _neg_llf(lmb, x):\n        return -llf(lmb, x)\n    return brent(_neg_llf, args=(x,))"
  },
  {
    "objectID": "blog/power-transform.html#numerical-instabilities",
    "href": "blog/power-transform.html#numerical-instabilities",
    "title": "Power Transform",
    "section": "2 Numerical Instabilities",
    "text": "2 Numerical Instabilities\nBecause both transformations involve exponentiation, they are susceptible to numerical overflow. This problem has been observed by (Marchand et al. 2022) and discussed in Scikit-learn’s GitHub Issue. Below is an example illustrating the problem using a naive implementation of Equation 3:\n\ndef boxcox_llf_naive(lmb, x):\n    n = len(x)\n    logx = np.log(x)\n    logvar = np.log(np.var(boxcox(x, lmb)))\n    return (lmb - 1) * np.sum(logx) - n/2 * logvar\n\n\nx = np.array([10, 10, 10, 9.9])\nprint(max_llf(x, llf=boxcox_llf_naive))\n\n156.48528753755807\n\n\nAlthough this returns a \\lambda value, it produces overflow warnings. A useful diagnostic is to visualize the log-likelihood curve:\n\n\nCode\ndef plot_llf(x, lb, ub, llf):\n    np.set_printoptions(precision=3)\n    lmb = np.linspace(lb, ub, 20)\n    ll = np.array([llf(l, x) for l in lmb])\n    print(f\"llf={ll}\")\n\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax.plot(lmb, ll)\n    ax.set_xlabel(r\"$\\lambda$\")\n    ax.set_ylabel(r\"$\\ln\\mathcal{L}(\\lambda, x)$\")\n    plt.show()\n\n\n\nplot_llf(x, lb=150, ub=200, llf=boxcox_llf_naive)\n\nllf=[13.684 13.697 13.711   -inf   -inf   -inf   -inf   -inf   -inf   -inf\n   -inf   -inf   -inf   -inf   -inf   -inf   -inf   -inf   -inf   -inf]\n\n\n\n\n\n\n\n\n\nUnfortunately, due to overflow (np.inf values), the log-likelihood curve cannot be visualized in the specified range, suggesting the returned \\lambda is not optimal."
  },
  {
    "objectID": "blog/power-transform.html#existing-solutions",
    "href": "blog/power-transform.html#existing-solutions",
    "title": "Power Transform",
    "section": "3 Existing Solutions",
    "text": "3 Existing Solutions\nThe MASS package in R (Venables and Ripley 2002) proposes a simple yet effective trick: divide the data by its mean. This rescales the data and avoids numerical instability without affecting the optimization outcome.\n\nx_dm = x / np.mean(x)\nprint(max_llf(x_dm, llf=boxcox_llf_naive))\n\n357.55141884289054\n\n\n\nplot_llf(x_dm, lb=330, ub=385, llf=boxcox_llf_naive)\n\nllf=[23.376 23.377 23.378 23.38  23.381 23.381 23.382 23.383 23.383 23.383\n 23.383 23.383 23.383 23.382 23.382 23.381 23.38  23.379 23.377 23.376]\n\n\n\n\n\n\n\n\n\nTo see why this works, consider the log-variance term for \\lambda \\ne 0:\n\n\\begin{align*}\n\\ln\\text{Var}[\\psi_{\\text{BC}}(\\lambda,x)]\n&=\\ln\\text{Var}[(x^\\lambda-1)/\\lambda] \\\\\n&=\\ln\\text{Var}[x^\\lambda/\\lambda] \\\\\n&=\\ln[\\text{Var}(x^\\lambda)/\\lambda^2] \\\\\n&=\\ln\\text{Var}(x^\\lambda) - 2\\ln|\\lambda| \\\\\n\\end{align*}\n\\tag{5}\nIf x is scaled by a constant c &gt; 0:\n\n\\begin{align*}\n\\ln\\text{Var}[\\psi_{\\text{BC}}(\\lambda,x/c)]\n&=\\ln\\text{Var}[(x/c)^\\lambda] - 2\\ln|\\lambda| \\\\\n&=\\ln[\\text{Var}(x^\\lambda)/c^{2\\lambda}] - 2\\ln|\\lambda| \\\\\n&=\\ln\\text{Var}(x^\\lambda) - 2\\lambda\\ln c  - 2\\ln|\\lambda| \\\\\n&=\\ln\\text{Var}[\\psi_{\\text{BC}}(\\lambda,x)] - 2\\lambda\\ln c \\\\\n\\end{align*}\n\\tag{6}\nPlugging into the log−likelihood:\n\n\\begin{align*}\n\\ln\\mathcal{L}_{\\text{BC}}(\\lambda, x/c)\n&=(\\lambda-1) \\sum_i^n \\ln(x_i/c) - \\frac{n}{2}\\ln\\text{Var}[\\psi_{\\text{BC}}(\\lambda,x/c)] \\\\\n&=\\ln\\mathcal{L}_{\\text{BC}}(\\lambda, x) - n(\\lambda-1)\\ln c + n\\lambda\\ln c \\\\\n&=\\ln\\mathcal{L}_{\\text{BC}}(\\lambda, x) + n\\ln c \\\\\n\\end{align*}\n\\tag{7}\nThe additive constant n\\ln c does not affect the maximizer of \\lambda. However, this trick does not apply to Yeo-Johnson, where scaling alters the optimal parameter (see Equation 4)."
  },
  {
    "objectID": "blog/power-transform.html#log-space-computation",
    "href": "blog/power-transform.html#log-space-computation",
    "title": "Power Transform",
    "section": "4 Log-Space Computation",
    "text": "4 Log-Space Computation\nTo address numerical instability for both transformations, log-space computation (Haberland 2023) is effective. It uses the Log-Sum-Exp trick to compute statistics in log space:\n\nfrom scipy.special import logsumexp\n\ndef log_mean(logx):\n    # compute log of mean of x from log(x)\n    return logsumexp(logx) - np.log(len(logx))\n\ndef log_var(logx):\n    # compute log of variance of x from log(x)\n    logmean = log_mean(logx)\n    pij = np.full_like(logx, np.pi * 1j, dtype=np.complex128)\n    logxmu = logsumexp([logx, logmean + pij], axis=0)\n    return np.real(logsumexp(2 * logxmu)) - np.log(len(logx))\n\nThis allows direct computation of \\ln\\sigma^2_\\psi from \\ln x. Plugging in Equation 5, we can compute the log-likelihood in the log-space.\n\ndef boxcox_llf(lmb, x):\n    n = len(x)\n    logx = np.log(x)\n    if lmb == 0:\n        logvar = np.log(np.var(logx))\n    else:\n        logvar = log_var(lmb * logx) - 2 * np.log(abs(lmb))\n    return (lmb - 1) * np.sum(logx) - n/2 * logvar\n\nThis version avoids overflow and reliably returns the optimal \\lambda.\n\nprint(max_llf(x, llf=boxcox_llf))\n\n357.55141245531865\n\n\n\nplot_llf(x, lb=330, ub=385, llf=boxcox_llf)\n\nllf=[14.175 14.177 14.178 14.179 14.18  14.181 14.182 14.182 14.183 14.183\n 14.183 14.183 14.182 14.182 14.181 14.18  14.179 14.178 14.177 14.176]\n\n\n\n\n\n\n\n\n\nThe same principle extends to Yeo-Johnson, even for mixed-sign inputs.\n\n\nCode\ndef log_var_yeojohnson(x, lmb):\n    if np.all(x &gt;= 0):\n        if abs(lmb) &lt; np.spacing(1.0):\n            return np.log(np.var(np.log1p(x)))\n        return log_var(lmb * np.log1p(x)) - 2 * np.log(abs(lmb))\n\n    elif np.all(x &lt; 0):\n        if abs(lmb - 2) &lt; np.spacing(1.0):\n            return np.log(np.var(np.log1p(-x)))\n        return log_var((2 - lmb) * np.log1p(-x)) - 2 * np.log(abs(2 - lmb))\n\n    else:  # mixed positive and negtive data\n        logyj = np.zeros_like(x, dtype=np.complex128)\n        pos = x &gt;= 0\n\n        # when x &gt;= 0\n        if abs(lmb) &lt; np.spacing(1.0):\n            logyj[pos] = np.log(np.log1p(x[pos]) + 0j)\n        else:  # lmbda != 0\n            logm1_pos = np.full_like(x[pos], np.pi * 1j, dtype=np.complex128)\n            logyj[pos] = logsumexp(\n                [lmb * np.log1p(x[pos]), logm1_pos], axis=0\n            ) - np.log(lmb + 0j)\n\n        # when x &lt; 0\n        if abs(lmb - 2) &lt; np.spacing(1.0):\n            logyj[~pos] = np.log(-np.log1p(-x[~pos]) + 0j)\n        else:  # lmbda != 2\n            logm1_neg = np.full_like(x[~pos], np.pi * 1j, dtype=np.complex128)\n            logyj[~pos] = logsumexp(\n                [(2 - lmb) * np.log1p(-x[~pos]), logm1_neg], axis=0\n            ) - np.log(lmb - 2 + 0j)\n\n        return log_var(logyj)\n\ndef yeojohnson_llf(lmb, x):\n    n = len(x)\n    llf = (lmb - 1) * np.sum(np.sign(x) * np.log1p(np.abs(x)))\n    llf += -n / 2 * log_var_yeojohnson(x, lmb)\n    return llf"
  },
  {
    "objectID": "blog/power-transform.html#constrained-optimization",
    "href": "blog/power-transform.html#constrained-optimization",
    "title": "Power Transform",
    "section": "5 Constrained Optimization",
    "text": "5 Constrained Optimization\nEven with log-space computation, applying the transformation using a large absolute value of \\lambda can result in overflow:\n\nlmax = max_llf(x, llf=boxcox_llf)\nprint(boxcox(x, lmax))\n\n[inf inf inf inf]\n\n\nTo prevent this, we constrain \\lambda so that the transformed values stay within a safe range.\n\nLemma 1 The transformation \\psi(\\lambda,x) is monotonically increasing in both \\lambda and x (see I.-K. Yeo 1997 for Yeo-Johnson proof).\n\nThis allows us to bound \\lambda based on \\min(x) and \\max(x). For Box-Cox, we use x=1 as a threshold since \\psi_\\text{BC}(\\lambda,1)=0:\n\n\\begin{align*}\n\\max_\\lambda \\quad & \\ln\\mathcal{L}_{\\text{BC}}(\\lambda, x) \\\\\n\\textrm{s.t.} \\quad & \\lambda\\le\\psi^{-1}_{\\text{BC}}(x_{\\max},y_{\\max}) & \\text{if } x_{\\max}&gt;1 \\\\\n& \\lambda\\ge\\psi^{-1}_{\\text{BC}}(x_{\\min},-y_{\\max}) & \\text{if } x_{\\min}&lt;1 \\\\\n\\end{align*}\n\\tag{8}\nThe inverse Box−Cox function is given by:\n\n\\psi^{-1}_{\\text{BC}}(x,y)=\n-\\tfrac{1}{y}-\\tfrac{1}{\\ln x}W\\left(-\\tfrac{x^{-1/y}\\ln x}{y}\\right)\n\\tag{9}\n\nfrom scipy.special import lambertw\n\ndef boxcox_inv_lmbda(x, y):\n    num = lambertw(-(x ** (-1 / y)) * np.log(x) / y, k=-1)\n    return -1 / y - np.real(num / np.log(x))\n\n\ndef boxcox_constranined_lmax(lmax, x, ymax):\n    # x &gt; 1, boxcox(x) &gt; 0; x &lt; 1, boxcox(x) &lt; 0\n    xmin, xmax = min(x), max(x)\n    if xmin &gt;= 1:\n        x_treme = xmax\n    elif xmax &lt;= 1:\n        x_treme = xmin\n    else:  # xmin &lt; 1 &lt; xmax\n        indicator = boxcox(xmax, lmax) &gt; abs(boxcox(xmin, lmax))\n        x_treme = xmax if indicator else xmin\n    \n    if abs(boxcox(x_treme, lmax)) &gt; ymax:\n        lmax = boxcox_inv_lmbda(x_treme, ymax * np.sign(x_treme - 1))\n    return lmax\n\nThis method can be verified by testing different values for y_{\\max}:\n\ndef verify_boxcox_constranined_lmax(x):\n    np.set_printoptions(precision=3)\n    lmax = max_llf(x, llf=boxcox_llf)\n    for ymax in [1e300, 1e100, 1e30, 1e10]:\n        l = boxcox_constranined_lmax(lmax, x, ymax)\n        print(boxcox(x, l))\n\n\n# Positive overflow\nx = np.array([10, 10, 10, 9.9])\nverify_boxcox_constranined_lmax(x)\n\n[1.000e+300 1.000e+300 1.000e+300 4.783e+298]\n[1.000e+100 1.000e+100 1.000e+100 3.587e+099]\n[1.000e+30 1.000e+30 1.000e+30 7.286e+29]\n[1.00e+10 1.00e+10 1.00e+10 8.95e+09]\n\n\n\n# Negative overflow\nx = np.array([0.1, 0.1, 0.1, 0.101])\nverify_boxcox_constranined_lmax(x)\n\n[-1.00e+300 -1.00e+300 -1.00e+300 -4.93e+298]\n[-1.000e+100 -1.000e+100 -1.000e+100 -3.624e+099]\n[-1.000e+30 -1.000e+30 -1.000e+30 -7.309e+29]\n[-1.000e+10 -1.000e+10 -1.000e+10 -8.959e+09]\n\n\nThe constrained optimization approach can also be extended to Yeo-Johnson, ensures overflow-free transformations even for extreme values.\n\n\nCode\ndef yeojohnson_inv_lmbda(x, y):\n    if x &gt;= 0:\n        num = lambertw(-((x + 1) ** (-1 / y) * np.log1p(x)) / y, k=-1)\n        return -1 / y + np.real(-num / np.log1p(x))\n    else:\n        num = lambertw(((1 - x) ** (1 / y) * np.log1p(-x)) / y, k=-1)\n        return -1 / y + 2 + np.real(num / np.log1p(-x))\n\ndef yeojohnson_constranined_lmax(lmax, x, ymax):\n    # x &gt; 0, yeojohnson(x) &gt; 0; x &lt; 0, yeojohnson(x) &lt; 0\n    xmin, xmax = min(x), max(x)\n    if xmin &gt;= 0:\n        x_treme = xmax\n    elif xmax &lt;= 0:\n        x_treme = xmin\n    else:  # xmin &lt; 0 &lt; xmax\n        with np.errstate(over=\"ignore\"):\n            indicator = yeojohnson(xmax, lmax) &gt; abs(yeojohnson(xmin, lmax))\n        x_treme = xmax if indicator else xmin\n\n    with np.errstate(over=\"ignore\"):\n        if abs(yeojohnson(x_treme, lmax)) &gt; ymax:\n            lmax = yeojohnson_inv_lmbda(x_treme, ymax * np.sign(x_treme))\n    return lmax"
  },
  {
    "objectID": "teach/index.html",
    "href": "teach/index.html",
    "title": "Xuefeng Xu",
    "section": "",
    "text": "Teaching Assistant\n\n@ University of Warwick\n\nCS275 Probability and Statistics: Autumn 2025\nCS429/CS909 Data Mining: Spring 2025\nCS133 Professional Skills: Autumn 2024\n\n\n\n@ Beihang University\n\n39112201 Abstract Algebra: Autumn 2019"
  }
]